# -*- coding: utf-8 -*-
"""__init__.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1bHcld6YrvpeBShLfOJ5w48xj3Yl2h6dF
"""
from xgboost import XGBClassifier
from sklearn.model_selection import cross_val_score
from sklearn.model_selection import KFold
from sklearn.metrics import confusion_matrix

from rdkit import Chem, DataStructs
from rdkit.Chem import AllChem, Draw, rdFMCS
from rdkit.Chem.AtomPairs import Pairs
from rdkit.Chem.Fingerprints import FingerprintMols
from IPython.display import display
from PIL import Image
from io import StringIO
import requests
from base64 import b64decode
import pandas as pd
from mordred import Calculator, descriptors, WienerIndex, ZagrebIndex
import statsmodels as sm
import param
import time
import matplotlib.pyplot as plt
from rdkit.Chem.Draw import IPythonConsole
from rdkit import Chem
from rdkit.Chem import rdFMCS
from rdkit.Chem import Draw
import pandas as pd
import requests
import time
from io import StringIO
from rdkit import Chem
from rdkit.Chem import AllChem, Draw
from rdkit import Chem
from rdkit.Chem import AllChem
from IPython.display import display
from rdkit import Chem
from rdkit.Chem import AllChem
from IPython.display import display
from rdkit.Chem import Draw
from rdkit import Chem
from rdkit.Chem import rdFMCS
from IPython.display import display
from rdkit.Chem import Draw
from rdkit import Chem
from rdkit.Chem import rdFMCS
from rdkit.Chem.Draw import IPythonConsole
from rdkit import Chem
from rdkit.Chem import rdFMCS, Draw, AllChem
from IPython.display import display
import requests
from rdkit import Chem, DataStructs
from rdkit.Chem import rdDepictor, Draw
from rdkit.Chem.AtomPairs import Pairs
from rdkit.Chem.Fingerprints import FingerprintMols
from IPython.display import display
from rdkit import Chem
from rdkit.Chem import rdFMCS
from rdkit.Chem import Draw
from IPython.display import display
import requests
from rdkit.Chem import Draw
from rdkit import Chem
from rdkit.Chem import rdFMCS
from rdkit.Chem.Draw import IPythonConsole
IPythonConsole.drawOptions.addAtomIndices = True
IPythonConsole.drawOptions.addStereoAnnotation = True
from rdkit import Chem
from rdkit.Chem import rdFMCS, Draw, AllChem
from IPython.display import display
import pandas as pd
from rdkit import Chem
from mordred import WienerIndex, ZagrebIndex
from statsmodels.api import OLS, add_constant
import statsmodels.api as sm
import matplotlib.pyplot as plt
import seaborn as sns
import pandas as pd
from sklearn.feature_selection import VarianceThreshold
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder
from lazypredict.Supervised import LazyRegressor
from lazypredict.Supervised import LazyRegressor  # Automated model selection and evaluation

# Import necessary libraries
import requests  # For making HTTP requests
import pandas as pd  # For handling data in DataFrame
from rdkit import Chem  # Chemistry toolkit for molecular structure handling
from mordred import Calculator, descriptors  # Molecular descriptors calculation
from sklearn.ensemble import RandomForestClassifier  # Random Forest Classifier
from sklearn.model_selection import train_test_split  # For splitting data into training and testing sets
from sklearn.metrics import accuracy_score, classification_report  # Model evaluation metrics
from sklearn.preprocessing import LabelEncoder  # For encoding categorical labels
from sklearn.impute import SimpleImputer  # For handling missing values
from sklearn.pipeline import make_pipeline  # Pipeline for chaining multiple estimators
from sklearn.preprocessing import StandardScaler  # For standardizing features
from sklearn.feature_selection import VarianceThreshold  # For removing low-variance features
from lazypredict.Supervised import LazyRegressor  # Automated model selection and evaluation
# Function to remove low-variance features from input data
from lazypredict.Supervised import LazyClassifier

def Intro():
    print("""
      In this project, I dive into the fascinating world where computational biology meets cheminformatics, driven by the desire to decode
      the intricate dance between chemicals and biology.

      Beginning with mining PubChem's extensive database and leveraging tools like RDKit, the project focuses on understanding the language
      of molecular structures. This foundational step sets the stage for in-depth Quantitative Structure-Activity Relationship (QSAR)
        analysis using Mordred, unveiling molecular properties critical for predictive modeling.

      The project takes a practical turn by applying machine learning models such as LazyRegressor and RandomForestClassifier. By dissecting
      chemical data, the goal is to not only evaluate models but also gain insights into the importance of various features.

      Venturing further, the integration of fingerprints and PubChem fingerprints (PCFP) into the machine learning framework introduces
      a novel dimension. This enables the project to decipher intricate patterns and relationships between molecular structures and
        biological activities.

        Applications:

    1.  Drug Discovery Acceleration:

        Example: Utilizing QSAR analysis to predict the biological activities of chemical compounds, streamlining the identification of potential drug candidates for specific bioassays (e.g., qspr_analysis function).

    2. Efficient Chemical Library Screening:

        Example: Employing machine learning models such as LazyRegressor to evaluate and rank molecular descriptors, facilitating the prioritization of chemical compounds for experimental testing (analyze_bioassay_ml function).

    3. Toxicity Prediction:

        Example: Leveraging molecular fingerprints and PCFP to predict toxicity levels, aiding in the identification of non-drug-like compounds and minimizing the risk associated with certain chemical structures (qsar_assay_fingerprint_analysis function).

    4. Environmental Risk Assessment:

        Example: Applying cheminformatics tools to assess the environmental impact of specific chemical structures, contributing to the understanding of potential ecological risks associated with chemical compounds (qsar_assay_fingerprint_analysis function).

    5. Molecular Design Optimization:

        Example: Extracting molecular descriptors and fingerprints to guide the design of novel compounds with desirable properties, enhancing the efficiency of molecular design processes (qspr_analysis and qsar_assay_fingerprint_analysis functions).

    6.  Scientific Insights into Chemical-Biological Relationships:

        Example: Uncovering intricate patterns and relationships between molecular structures and biological activities through comprehensive data analysis, offering valuable insights for scientific research (analyze_bioassay_ml and qsar_assay_fingerprint_analysis functions).

    7. Resource Saving in Experimental Validation:

        Example: Prioritizing chemical compounds based on predictive models, reducing the need for extensive experimental validation and conserving resources in the drug discovery pipeline (analyze_bioassay_ml function).

    8. Contributions to Computational Biology:

        Example: Integrating cheminformatics and machine learning to advance computational biology, offering a versatile toolkit for researchers and scientists exploring the connections between chemicals and biology (entire project scope).
        This project, fueled by its diverse functions, serves as a valuable asset in advancing scientific research, particularly in the fields of drug discovery, molecular design, and environmental impact assessment. The applications extend far beyond the code, promising transformative contributions to the realms of computational biology and cheminformatics.


          """)

    print("**Functions Summary:**\n")
    functions = [
        ("Display Active Chemicals by Assay ID", "display_active_chemicals_by_assay_aid(aid)", [
            "Retrieves active compound CIDs for a specified assay.",
            "Displays the common substructure of the active compounds using molecular images.",
            "Returns the URL for the active compound CID data."
        ]),
        ("Display Inactive Chemicals by Assay ID", "display_inactive_chemicals_by_assay_aid(aid)", [
            "Retrieves inactive compound CIDs for a specified assay.",
            "Displays the common substructure of the inactive compounds using molecular images.",
            "Returns the URL for the inactive compound CID data."
        ]),
        ("Retrieve Active CIDs by Assay ID", "retrieve_active_cids_by_assay_aid(assay_id)", [
            "Fetches and prints the active compound CIDs for a given assay ID.",
            "Returns the URL for active compounds."
        ]),
        ("Retrieve Inactive CIDs by Assay ID", "retrieve_inactive_cids_by_assay_aid(assay_id)", [
            "Fetches and prints the inactive compound CIDs for a given assay ID.",
            "Returns the URL for inactive compounds."
        ]),
        ("Retrieve Active SIDs by Substance ID", "retrieve_active_sids_by_substance_aid(substance_id)", [
            "Fetches and prints the active sample IDs (SIDs) for a given substance ID.",
            "Returns the URL for active SIDs."
        ]),
        ("Retrieve Inactive SIDs by Substance ID", "retrieve_inactive_sids_by_substance_aid(substance_id)", [
            "Fetches and prints the inactive sample IDs (SIDs) for a given substance ID.",
            "Returns the URL for inactive SIDs."
        ]),
        ("Virtual Screening of Chemical Compounds", "virtual_screening(file_path, *colnames)", [
            "Performs virtual screening on chemical compounds from a CSV file.",
            "Conducts a similarity search against PubChem and filters compounds based on Lipinski's rule of five.",
            "Displays and saves the top 10 compounds that satisfy screening criteria.",
            "Generates a detailed CSV file, 'Virtual_Screening.csv,' with information about the screened compounds."
        ]),
        ("QSPR Analysis with Multiple Descriptors", "qspr_analysis(csv_file_path, dependent_variable_column)", [
            "Calculates various molecular descriptors (Wiener index, Zagreb indices) for compounds in a CSV file.",
            "Performs a QSPR (Quantitative Structure-Property Relationship) analysis to predict a dependent variable.",
            "Saves the analysis results, including descriptor values, in a new CSV file.",
            "Outputs a model summary text file.",
            "Displays the analysis results and the model summary."
        ]),
        ("QSPR Analysis with User-Specified Descriptors", "qspr_analysis_2(csv_file_path, dependent_variable_column, descriptor_types)", [
            "Calculates user-specified molecular descriptors (Wiener index, Zagreb indices) for compounds in a CSV file.",            "Allows users to choose specific descriptors for the analysis.",
            "Performs a QSPR analysis to predict a dependent variable.",
            "Saves the analysis results, including descriptor values, in a new CSV file.",
            "Outputs a model summary text file.",
            "Displays the analysis results and the model summary."
        ]),
        ("Predict Candidate Active Chemicals (Version 1)", "predict_candidate_active_chemicals(assay_id)", [
            "Retrieves active compound CIDs for a specified assay and their corresponding SMILES.",
            "Performs a fast 2D similarity search for potential candidate active compounds.",
            "Filters out non-drug-like compounds based on specific criteria.",
            "Displays the top 10 candidate compounds' structures and saves the results in a CSV file."
        ]),
        ("Predict Candidate Active Chemicals (Version 2)", "predict_candidate_active_chemicals2(assay_id)", [
            "A modified version of the previous function with the same purpose.",
            "Retrieves active compound CIDs for a specified assay and their corresponding SMILES.",
            "Performs a fast 2D similarity search for potential candidate active compounds.",
            "Not Filters out non-drug-like compounds based on specific criteria.",
            "Displays the top 10 candidate compounds' structures and saves the results in a CSV file."
        ]),
        ("Analyze Bioassay Using Machine Learning", "analyze_bioassay_ml(aid, output_file=None)", [
            "Analyzes a bioassay using machine learning techniques.",
            "Retrieves active and inactive compound CIDs for a specified assay.",
            "Converts CIDs to Isomeric SMILES and creates a DataFrame with assay information.",
            "Generates molecular descriptors using the Mordred library for the compounds.",
            "Merges the assay information DataFrame with the descriptor DataFrame.",
            "Encodes the 'Status' column to binary values (1 for active, 0 for inactive).",
            "Removes low variance features from the data.",
            "Splits the data into training and testing sets.",
            "Uses LazyRegressor to build and evaluate multiple machine learning models on the training set.",
            "Displays a performance table for the training set and saves it to a CSV file.",
            "Generates bar plots for R-squared, RMSE, and Time Taken metrics for model evaluation."
        ]),
        ("QSAR Assay Fingerprint Analysis", "qsar_assay_fingerprint_analysis(aid_input)", [
            "Performs QSAR (Quantitative Structure-Activity Relationship) assay analysis using fingerprints and PCFP (PubChem Fingerprint).",
            "Retrieves active compound CIDs for a specified assay along with their fingerprints and PCFP.",
            "Creates a DataFrame with CID, Activity status, 2D Fingerprint, and PCFP columns.",
            "Encodes the 'Activity' column to binary values (0 for inactive, 1 for active).",
            "Drops columns 'CID' and 'Fingerprint2D' for modeling.",
            "Splits the data into training, validation, and testing sets.",
            "Uses LazyRegressor to build and evaluate multiple regression models.",
            "Displays the R-squared, RMSE, and Time Taken metrics for model evaluation.",
            "Saves bar plots of R-squared, RMSE, and Time Taken metrics."
        ])
    ]
    for func_name, func_signature, func_description in functions:
        print(f"\n{func_name}\n{func_signature}\n")
        for point in func_description:
            print(f"  - {point}")

    print("""
    For more information, refer to the documentation available in the following link:
    https://docs.google.com/document/d/1noygWmUiBCPmNXH-M6Jun3yiKjlsgPaa3lzXZBbOENE/edit?usp=sharing

    The source code for this project is available on GitHub:
    https://github.com/Ahmed212517329/pubcem.git

    For inquiries, you can contact the author:
    Author: Ahmed Alhilal
    Email: aalhilal@kfu.edu.sa
    """)



    return
Intro()

def display_active_chemicals_by_assay_aid(aid):
    """
    Retrieves inactive compound CIDs from a specified assay and displays the common substructure.

    Parameters:
    - aid (int): Assay ID.

    Returns:
    - str: URL of the inactive compound CID data.
    """

    study_url = f"https://pubchem.ncbi.nlm.nih.gov/rest/pug/assay/aid/{aid}/cids/txt?cids_type=active"
    url = requests.get(study_url)
    cids = url.text.split()
    str_cid = ",".join(str(x) for x in cids)

    compound_url = f"https://pubchem.ncbi.nlm.nih.gov/rest/pug/compound/cid/{str_cid}/property/IsomericSMILES/txt"
    res = requests.get(compound_url)
    ms = res.text.split()
    molecules = list(map(Chem.MolFromSmiles, ms))
    grid_image = Chem.Draw.MolsToGridImage(molecules, subImgSize=(400, 400))
    display(grid_image)

    mcs_result = rdFMCS.FindMCS(molecules, threshold=0.7)
    common_substructure = Chem.MolFromSmarts(mcs_result.smartsString)

    print("The common substructure for these CIDs:")
    display(common_substructure)
    return compound_url


def display_inactive_chemicals_by_assay_aid(aid):
    """
    Retrieves inactive compound CIDs from a specified assay and displays the common substructure.

    Parameters:
    - aid (int): Assay ID.

    Returns:
    - str: URL of the inactive compound CID data.
    """

    study_url = f"https://pubchem.ncbi.nlm.nih.gov/rest/pug/assay/aid/{aid}/cids/txt?cids_type=inactive"
    url = requests.get(study_url)
    cids = url.text.split()
    str_cid = ",".join(str(x) for x in cids)

    compound_url = f"https://pubchem.ncbi.nlm.nih.gov/rest/pug/compound/cid/{str_cid}/property/IsomericSMILES/txt"
    res = requests.get(compound_url)
    ms = res.text.split()
    molecules = list(map(Chem.MolFromSmiles, ms))
    grid_image = Chem.Draw.MolsToGridImage(molecules, subImgSize=(400, 400))
    display(grid_image)

    mcs_result = rdFMCS.FindMCS(molecules, threshold=0.7)
    common_substructure = Chem.MolFromSmarts(mcs_result.smartsString)

    print("The common substructure for these CIDs:")
    display(common_substructure)
    return compound_url





def retrieve_active_cids_by_assay_aid(assay_id):
    """
    Fetches and prints the active compound CIDs for a given assay ID.

    Parameters:
    - assay_id (int): Assay ID for PubChem.

    Returns:
    - str: URL for active compounds.
    """
    rdDepictor.SetPreferCoordGen(True)
    Draw.DrawingOptions.addAtomIndices = True
    Draw.DrawingOptions.addStereoAnnotation = True

    assay_id = str(assay_id)
    active_url = f"https://pubchem.ncbi.nlm.nih.gov/rest/pug/assay/aid/{assay_id}/cids/txt?cids_type=active"
    response = requests.get(active_url)
    active_cids = response.text.split()

    print("Active compound CIDs:\n", active_cids)
    return active_url


def retrieve_inactive_cids_by_assay_aid(assay_id):
    """
    Fetches and prints the inactive compound CIDs for a given assay ID.

    Parameters:
    - assay_id (int): Assay ID for PubChem.

    Returns:
    - str: URL for inactive compounds.
    """
    rdDepictor.SetPreferCoordGen(True)
    Draw.DrawingOptions.minFontSize = 20

    assay_id = str(assay_id)
    inactive_url = f"https://pubchem.ncbi.nlm.nih.gov/rest/pug/assay/aid/{assay_id}/cids/txt?cids_type=inactive"
    response = requests.get(inactive_url)
    inactive_cids = response.text.split()

    print("Inactive compound CIDs:\n", inactive_cids)
    return inactive_url


def retrieve_active_sids_by_substance_aid(substance_id):
    """
    Fetches and prints the active sample IDs (SIDs) for a given substance ID.

    Parameters:
    - substance_id (int): Substance ID for PubChem.

    Returns:
    - str: URL for active SIDs.
    """
    # Your drawing options or other settings here
    substance_id = str(substance_id)
    active_url = f"https://pubchem.ncbi.nlm.nih.gov/rest/pug/assay/aid/{substance_id}/sids/txt?sids_type=active"
    response = requests.get(active_url)
    active_sids = response.text.split()

    print("Active sample SIDs:\n", active_sids)
    return active_url

def retrieve_inactive_sids_by_substance_aid(substance_id):
    """
    Fetches and prints the inactive sample IDs (SIDs) for a given substance ID.

    Parameters:
    - substance_id (int): Substance ID for PubChem.

    Returns:
    - str: URL for inactive SIDs.
    """
    # Your drawing options or other settings here
    substance_id = str(substance_id)
    inactive_url = f"https://pubchem.ncbi.nlm.nih.gov/rest/pug/assay/aid/{substance_id}/sids/txt?sids_type=inactive"
    response = requests.get(inactive_url)
    inactive_sids = response.text.split()

    print("Inactive sample SIDs:\n", inactive_sids)
    return inactive_url
import pandas as pd
from rdkit import Chem
from rdkit.Chem import Descriptors
from rdkit.Chem import rdMolDescriptors
import statsmodels.api as sm

def qspr_analysis(csv_file_path, smiles_column_name, dependent_variable_column, additional_independent_variable=None):
    # Create descriptor instances
    zagreb_index1 = rdMolDescriptors.CalcNumAromaticRings
    zagreb_index2 = rdMolDescriptors.CalcNumRotatableBonds
    wiener_index = Descriptors.MolWt

    # Read the CSV file into a pandas dataframe
    df = pd.read_csv(csv_file_path)
    display(df)

    # Create lists to store calculated results
    result_Wiener = []
    result_Z1 = []
    result_Z2 = []

    # Iterate through each row of the CSV data
    for index, row in df.iterrows():
        SMILE = row[smiles_column_name]
        mol = Chem.MolFromSmiles(SMILE)

        # Calculate descriptor values
        result_Wiener.append(wiener_index(mol))
        result_Z1.append(zagreb_index1(mol))
        result_Z2.append(zagreb_index2(mol))

    # Add the calculated results to the dataframe
    df['Wiener'] = result_Wiener
    df['Z1'] = result_Z1
    df['Z2'] = result_Z2

    # Save the dataframe as a CSV file
    output_csv_path = f"{csv_file_path}_W_Z1_Z2_qspr_analysis_results.csv"
    df.to_csv(output_csv_path, index=False)
    display(df)

    # Perform QSPR analysis
    independent_variables = [ "Wiener", "Z1", "Z2"]
    if additional_independent_variable:
        independent_variables.append(additional_independent_variable)
    X = df[independent_variables]        # select our independent variables
    X = sm.add_constant(X)               # add an intercept to our model
    y = df[[dependent_variable_column]]  # select the dependent variable
    model = sm.OLS(y, X).fit()            # set up our model

    # Save the model summary to a text file
    output_text_path = f"{csv_file_path}_W_Z1_Z2_model_summary.txt"
    with open(output_text_path, "w") as text_file:
        text_file.write(model.summary().as_text())
        print(model.summary())

    print(f"\n***Results saved to:\n- CSV file: {output_csv_path}\n- Model summary text file: {output_text_path}")

    result = f"\n***Results saved to:\n- CSV file: {output_csv_path}\n- Model summary text file: {output_text_path}"
    return result

# Example usage:
# qspr_analysis("BPP.csv","SS","BP_K","MW")


import pandas as pd
from rdkit import Chem
from rdkit.Chem import Descriptors
from rdkit.Chem import rdMolDescriptors
import statsmodels.api as sm

def qspr_analysis_2(csv_file_path, dependent_variable_column, smiles_column_name, descriptor_types, additional_independent_variables=None):
    # Create descriptor instances based on user's choice
    descriptor_instances = []
    for descriptor_type in descriptor_types:
        if descriptor_type == "z1":
            descriptor_instances.append(rdMolDescriptors.CalcNumAromaticRings)
        elif descriptor_type == "z2":
            descriptor_instances.append(rdMolDescriptors.CalcNumRotatableBonds)
        elif descriptor_type == "w":
            descriptor_instances.append(Descriptors.MolWt)
        else:
            raise ValueError(f"Invalid descriptor type '{descriptor_type}'. Choose 'z1', 'z2', or 'w'.")

    # Read the CSV file into a pandas dataframe
    df = pd.read_csv(csv_file_path)

    # Create lists to store calculated results
    for descriptor_type, descriptor_instance in zip(descriptor_types, descriptor_instances):
        result_descriptor = []

        # Iterate through each row of the CSV data
        for index, row in df.iterrows():
            SMILE = row[smiles_column_name]
            mol = Chem.MolFromSmiles(SMILE)

            # Calculate descriptor values
            result_descriptor.append(descriptor_instance(mol))

        # Add the calculated results to the dataframe
        df[descriptor_type.capitalize()] = result_descriptor

    # Calculate additional independent variables if provided
    if additional_independent_variables:
        if "MW" in additional_independent_variables:
            mw_values = []
            for index, row in df.iterrows():
                SMILE = row[smiles_column_name]
                mol = Chem.MolFromSmiles(SMILE)
                mw_values.append(Descriptors.MolWt(mol))
            df["MW"] = mw_values

    # Save the dataframe as a CSV file
    output_csv_path = f"{csv_file_path}_qspr_analysis_results.csv"
    df.to_csv(output_csv_path, index=False)
    display(df)

    # Perform QSPR analysis
    independent_variables = [desc.capitalize() for desc in descriptor_types]
    if additional_independent_variables:
        independent_variables += ["MW"]
    X = df[independent_variables]        # select our independent variables
    X = sm.add_constant(X)               # add an intercept to our model
    y = df[[dependent_variable_column]]  # select the dependent variable
    model = sm.OLS(y, X).fit()            # set up our model

    # Save the model summary to a text file
    output_text_path = f"{csv_file_path}_model_summary.txt"
    with open(output_text_path, "w") as text_file:
        text_file.write(model.summary().as_text())
        print(model.summary())

    print(f"\n***Results saved to:\n- CSV file: {output_csv_path}\n- Model summary text file: {output_text_path}")
    result = (f"\n***Results saved to:\n- CSV file: {output_csv_path}\n- Model summary text file: {output_text_path}")
    return str(result)

# Example usage:
#qspr_analysis_2('BPP.csv', 'BP_K', 'SS', ['z1', 'z2', 'w'], ["MW"])


def predict_candidate_active_chemicals(assay_id):
    # Step 1: Retrieve CIDs from the first link
    cid_url = f"https://pubchem.ncbi.nlm.nih.gov/rest/pug/assay/aid/{assay_id}/cids/txt?cids_type=active"
    cid_response = requests.get(cid_url)
    cid_list = cid_response.text.split()

    # Step 2: Loop over CIDs to retrieve SMILES
    data = []

    for cid in cid_list:
        smiles_url = f"https://pubchem.ncbi.nlm.nih.gov/rest/pug/compound/cid/{cid}/property/IsomericSMILES/txt"
        smiles_response = requests.get(smiles_url)
        smiles = smiles_response.text.strip()

        data.append({'CID': cid, 'IsomericSMILES': smiles})

    # Step 3: Save data to DataFrame and then to a CSV file
    df = pd.DataFrame(data)
    df.to_csv(f"output_{assay_id}.csv", index=False)
    display(df)

    # Step 4: Perform similarity search
    prolog = "https://pubchem.ncbi.nlm.nih.gov/rest/pug"
    cids_hit = dict()

    for idx, mysmiles in enumerate(df['IsomericSMILES']):
        mydata = {'smiles': mysmiles}
        url = prolog + "/compound/fastsimilarity_2d/smiles/cids/txt"
        res = requests.post(url, data=mydata)

        if res.status_code == 200:
            cids = res.text.split()
            cids = [int(x) for x in cids]    # Convert CIDs from string to integer.
        else:
            print("Error at", idx, ":", df.loc[idx, 'CID'], mysmiles)
            print(res.status_code)
            print(res.content)

        for mycid in cids:
            cids_hit[mycid] = cids_hit.get(mycid, 0) + 1

        time.sleep(0.2)

    # Step 5: Exclude query compounds from hits
    cids_query = dict()

    for idx, mysmiles in enumerate(df['IsomericSMILES']):
        mydata = {'smiles': mysmiles}
        url = prolog + "/compound/fastidentity/smiles/cids/txt?identity_type=same_connectivity"
        res = requests.post(url, data=mydata)

        if res.status_code == 200:
            cids = res.text.split()
            cids = [int(x) for x in cids]
        else:
            print("Error at", idx, ":", df.loc[idx, 'CID'], mysmiles)
            print(res.status_code)
            print(res.content)

        for mycid in cids:
            cids_query[mycid] = cids_query.get(mycid, 0) + 1

        time.sleep(0.2)

    # Step 6: Exclude query compounds from hits
    cids_hit = {k: v for k, v in cids_hit.items() if k not in cids_query}

    # Step 7: Filtering out non-drug-like compounds
    chunk_size = 100

    if len(cids_hit) % chunk_size == 0:
        num_chunks = len(cids_hit) // chunk_size
    else:
        num_chunks = len(cids_hit) // chunk_size + 1

    cids_list = list(cids_hit.keys())

    print("# Number of chunks:", num_chunks)

    csv = ""   # sets a variable called csv to save the comma-separated output

    for i in range(num_chunks):
        print(i, end=" ")

        idx1 = chunk_size * i
        idx2 = chunk_size * (i + 1)

        cids_str = ",".join([str(x) for x in cids_list[idx1:idx2]])  # build pug input for chunks of data
        url = prolog + f"/compound/cid/{cids_str}/property/HBondDonorCount,HBondAcceptorCount,MolecularWeight,XLogP,CanonicalSMILES,IsomericSMILES/csv"

        res = requests.get(url)

        if i == 0:  # if this is the first request, store result in an empty csv variable
            csv = res.text
        else:          # if this is a subsequent request, add the request to the csv variable adding a new line between chunks
            csv = csv + "\n".join(res.text.split()[1:]) + "\n"

        time.sleep(0.2)

    # Step 8: Downloaded data (in CSV) are loaded into a pandas data frame
    csv_file = StringIO(csv)
    df_raw = pd.read_csv(csv_file, sep=",")

    # Step 9: Show the shape (dimensions) of the data frame
    print("DataFrame Shape:", df_raw.shape)

    # Step 10: First load the cids_hit dictionary into a data frame
    df_freq = pd.DataFrame(cids_hit.items(), columns=['CID', 'HitFreq'])
    df_freq.head(5)

    # Step 11: Create a new data frame called "df" by joining the df and df_freq data frames
    df = df_raw.join(df_freq.set_index('CID'), on='CID')
    df.shape
    df.sort_values(by=['HitFreq', 'CID'], ascending=False).head(10)

    # Step 12: Filter out non-drug-like compounds
    df = df[(df['HBondDonorCount'] <= 5) &
            (df['HBondAcceptorCount'] <= 10) &
            (df['MolecularWeight'] <= 500) &
            (df['XLogP'] < 5)]

    # Step 13: Draw the structures of the top 10 compounds
    cids_top = df.sort_values(by=['HitFreq', 'CID'], ascending=False).head(10).CID.to_list()
    # Save the original DataFrame to a CSV file
    df.to_csv(f'Prediction candiate active chemical for with filter out non_drug{assay_id}.csv', index=False)

    mols = []

    for mycid in cids_top:
        mysmiles = df[df.CID == mycid].IsomericSMILES.item()
        mol = Chem.MolFromSmiles(mysmiles)
        Chem.FindPotentialStereoBonds(mol)  # Identify potential stereo bonds!
        mols.append(mol)

    mylegends = ["CID " + str(x) for x in cids_top]
    img = Draw.MolsToGridImage(mols, molsPerRow=2, subImgSize=(400, 400), legends=mylegends)
    display(img)
    # Save the image to a PNG file
    fil=str(f'Prediction candiate active chemical is saved Prediction candiate active chemical for with filter out non_drug{assay_id}.csv')
    print(fil)
    return

def predict_candidate_active_chemicals2(assay_id):
    # Step 1: Retrieve CIDs from the first link
    cid_url = f"https://pubchem.ncbi.nlm.nih.gov/rest/pug/assay/aid/{assay_id}/cids/txt?cids_type=active"
    cid_response = requests.get(cid_url)
    cid_list = cid_response.text.split()

    # Step 2: Loop over CIDs to retrieve SMILES
    data = []

    for cid in cid_list:
        smiles_url = f"https://pubchem.ncbi.nlm.nih.gov/rest/pug/compound/cid/{cid}/property/IsomericSMILES/txt"
        smiles_response = requests.get(smiles_url)
        smiles = smiles_response.text.strip()

        data.append({'CID': cid, 'IsomericSMILES': smiles})

    # Step 3: Save data to DataFrame and then to a CSV file
    df = pd.DataFrame(data)
    df.to_csv(f"output_{assay_id}.csv", index=False)
    display(df)

    # Step 4: Perform similarity search
    prolog = "https://pubchem.ncbi.nlm.nih.gov/rest/pug"
    cids_hit = dict()

    for idx, mysmiles in enumerate(df['IsomericSMILES']):
        mydata = {'smiles': mysmiles}
        url = prolog + "/compound/fastsimilarity_2d/smiles/cids/txt"
        res = requests.post(url, data=mydata)

        if res.status_code == 200:
            cids = res.text.split()
            cids = [int(x) for x in cids]    # Convert CIDs from string to integer.
        else:
            print("Error at", idx, ":", df.loc[idx, 'CID'], mysmiles)
            print(res.status_code)
            print(res.content)

        for mycid in cids:
            cids_hit[mycid] = cids_hit.get(mycid, 0) + 1

        time.sleep(0.2)

    # Step 5: Exclude query compounds from hits
    cids_query = dict()

    for idx, mysmiles in enumerate(df['IsomericSMILES']):
        mydata = {'smiles': mysmiles}
        url = prolog + "/compound/fastidentity/smiles/cids/txt?identity_type=same_connectivity"
        res = requests.post(url, data=mydata)

        if res.status_code == 200:
            cids = res.text.split()
            cids = [int(x) for x in cids]
        else:
            print("Error at", idx, ":", df.loc[idx, 'CID'], mysmiles)
            print(res.status_code)
            print(res.content)

        for mycid in cids:
            cids_query[mycid] = cids_query.get(mycid, 0) + 1

        time.sleep(0.2)

    # Step 6: Exclude query compounds from hits
    cids_hit = {k: v for k, v in cids_hit.items() if k not in cids_query}

    # Step 7: Filtering out non-drug-like compounds
    chunk_size = 100

    if len(cids_hit) % chunk_size == 0:
        num_chunks = len(cids_hit) // chunk_size
    else:
        num_chunks = len(cids_hit) // chunk_size + 1

    cids_list = list(cids_hit.keys())

    print("# Number of chunks:", num_chunks)

    csv = ""   # sets a variable called csv to save the comma-separated output

    for i in range(num_chunks):
        print(i, end=" ")

        idx1 = chunk_size * i
        idx2 = chunk_size * (i + 1)

        cids_str = ",".join([str(x) for x in cids_list[idx1:idx2]])  # build pug input for chunks of data
        url = prolog + f"/compound/cid/{cids_str}/property/HBondDonorCount,HBondAcceptorCount,MolecularWeight,XLogP,CanonicalSMILES,IsomericSMILES/csv"

        res = requests.get(url)

        if i == 0:  # if this is the first request, store result in an empty csv variable
            csv = res.text
        else:          # if this is a subsequent request, add the request to the csv variable adding a new line between chunks
            csv = csv + "\n".join(res.text.split()[1:]) + "\n"

        time.sleep(0.2)

    # Step 8: Downloaded data (in CSV) are loaded into a pandas data frame
    csv_file = StringIO(csv)
    df_raw = pd.read_csv(csv_file, sep=",")

    # Step 9: Show the shape (dimensions) of the data frame
    print("DataFrame Shape:", df_raw.shape)

    # Step 10: First load the cids_hit dictionary into a data frame
    df_freq = pd.DataFrame(cids_hit.items(), columns=['CID', 'HitFreq'])
    df_freq.head(5)

    # Step 11: Create a new data frame called "df" by joining the df and df_freq data frames
    df = df_raw.join(df_freq.set_index('CID'), on='CID')
    df.shape
    df.sort_values(by=['HitFreq', 'CID'], ascending=False).head(10)

    # Step 13: Draw the structures of the top 10 compounds
    cids_top = df.sort_values(by=['HitFreq', 'CID'], ascending=False).head(10).CID.to_list()
    # Save the original DataFrame to a CSV file
    df.to_csv(f'Predict candiate active chemical without filter no drug for {assay_id} bioassay AID.csv', index=False)
    fil = str(f'Predict candiate active chemical without filter no drug for {assay_id} bioassay AID.csv')
    print(" it is save your file ", fil)
    mols = []

    for mycid in cids_top:
        mysmiles = df[df.CID == mycid].IsomericSMILES.item()
        mol = Chem.MolFromSmiles(mysmiles)
        Chem.FindPotentialStereoBonds(mol)  # Identify potential stereo bonds!
        mols.append(mol)

    mylegends = ["CID " + str(x) for x in cids_top]
    img = Draw.MolsToGridImage(mols, molsPerRow=2, subImgSize=(400, 400), legends=mylegends)
    display(img)
    return

def predict_candidate_active_chemicals(assay_id):
    # Step 1: Retrieve CIDs from the first link
    cid_url = f"https://pubchem.ncbi.nlm.nih.gov/rest/pug/assay/aid/{assay_id}/cids/txt?cids_type=active"
    cid_response = requests.get(cid_url)
    cid_list = cid_response.text.split()

    # Step 2: Loop over CIDs to retrieve SMILES
    data = []

    for cid in cid_list:
        smiles_url = f"https://pubchem.ncbi.nlm.nih.gov/rest/pug/compound/cid/{cid}/property/IsomericSMILES/txt"
        smiles_response = requests.get(smiles_url)
        smiles = smiles_response.text.strip()

        data.append({'CID': cid, 'IsomericSMILES': smiles})

    # Step 3: Save data to DataFrame and then to a CSV file
    df = pd.DataFrame(data)
    df.to_csv(f"output_{assay_id}.csv", index=False)
    display(df)

    # Step 4: Perform similarity search
    prolog = "https://pubchem.ncbi.nlm.nih.gov/rest/pug"
    cids_hit = dict()

    for idx, mysmiles in enumerate(df['IsomericSMILES']):
        mydata = {'smiles': mysmiles}
        url = prolog + "/compound/fastsimilarity_2d/smiles/cids/txt"
        res = requests.post(url, data=mydata)

        if res.status_code == 200:
            cids = res.text.split()
            cids = [int(x) for x in cids]    # Convert CIDs from string to integer.
        else:
            print("Error at", idx, ":", df.loc[idx, 'CID'], mysmiles)
            print(res.status_code)
            print(res.content)

        for mycid in cids:
            cids_hit[mycid] = cids_hit.get(mycid, 0) + 1

        time.sleep(0.2)

    # Step 5: Exclude query compounds from hits
    cids_query = dict()

    for idx, mysmiles in enumerate(df['IsomericSMILES']):
        mydata = {'smiles': mysmiles}
        url = prolog + "/compound/fastidentity/smiles/cids/txt?identity_type=same_connectivity"
        res = requests.post(url, data=mydata)

        if res.status_code == 200:
            cids = res.text.split()
            cids = [int(x) for x in cids]
        else:
            print("Error at", idx, ":", df.loc[idx, 'CID'], mysmiles)
            print(res.status_code)
            print(res.content)

        for mycid in cids:
            cids_query[mycid] = cids_query.get(mycid, 0) + 1

        time.sleep(0.2)

    # Step 6: Exclude query compounds from hits
    cids_hit = {k: v for k, v in cids_hit.items() if k not in cids_query}

    # Step 7: Filtering out non-drug-like compounds
    chunk_size = 100

    if len(cids_hit) % chunk_size == 0:
        num_chunks = len(cids_hit) // chunk_size
    else:
        num_chunks = len(cids_hit) // chunk_size + 1

    cids_list = list(cids_hit.keys())

    print("# Number of chunks:", num_chunks)

    csv = ""   # sets a variable called csv to save the comma-separated output

    for i in range(num_chunks):
        print(i, end=" ")

        idx1 = chunk_size * i
        idx2 = chunk_size * (i + 1)

        cids_str = ",".join([str(x) for x in cids_list[idx1:idx2]])  # build pug input for chunks of data
        url = prolog + f"/compound/cid/{cids_str}/property/HBondDonorCount,HBondAcceptorCount,MolecularWeight,XLogP,CanonicalSMILES,IsomericSMILES/csv"

        res = requests.get(url)

        if i == 0:  # if this is the first request, store result in an empty csv variable
            csv = res.text
        else:          # if this is a subsequent request, add the request to the csv variable adding a new line between chunks
            csv = csv + "\n".join(res.text.split()[1:]) + "\n"

        time.sleep(0.2)

    # Step 8: Downloaded data (in CSV) are loaded into a pandas data frame
    csv_file = StringIO(csv)
    df_raw = pd.read_csv(csv_file, sep=",")

    # Step 9: Show the shape (dimensions) of the data frame
    print("DataFrame Shape:", df_raw.shape)

    # Step 10: First load the cids_hit dictionary into a data frame
    df_freq = pd.DataFrame(cids_hit.items(), columns=['CID', 'HitFreq'])
    df_freq.head(5)

    # Step 11: Create a new data frame called "df" by joining the df and df_freq data frames
    df = df_raw.join(df_freq.set_index('CID'), on='CID')
    df.shape
    df.sort_values(by=['HitFreq', 'CID'], ascending=False).head(10)

    # Step 12: Filter out non-drug-like compounds
    df = df[(df['HBondDonorCount'] <= 5) &
            (df['HBondAcceptorCount'] <= 10) &
            (df['MolecularWeight'] <= 500) &
            (df['XLogP'] < 5)]

    # Step 13: Draw the structures of the top 10 compounds
    cids_top = df.sort_values(by=['HitFreq', 'CID'], ascending=False).head(10).CID.to_list()
    # Save the original DataFrame to a CSV file
    df.to_csv(f'Prediction candiate active chemical for with filter out non_drug{assay_id}.csv', index=False)

    mols = []

    for mycid in cids_top:
        mysmiles = df[df.CID == mycid].IsomericSMILES.item()
        mol = Chem.MolFromSmiles(mysmiles)
        Chem.FindPotentialStereoBonds(mol)  # Identify potential stereo bonds!
        mols.append(mol)

    mylegends = ["CID " + str(x) for x in cids_top]
    img = Draw.MolsToGridImage(mols, molsPerRow=2, subImgSize=(400, 400), legends=mylegends)
    display(img)
    # Save the image to a PNG file
    fil=str(f'Prediction candiate active chemical is saved Prediction candiate active chemical for with filter out non_drug{assay_id}.csv')
    print(fil)
    return

def predict_candidate_active_chemicals2(assay_id):
    # Step 1: Retrieve CIDs from the first link
    cid_url = f"https://pubchem.ncbi.nlm.nih.gov/rest/pug/assay/aid/{assay_id}/cids/txt?cids_type=active"
    cid_response = requests.get(cid_url)
    cid_list = cid_response.text.split()

    # Step 2: Loop over CIDs to retrieve SMILES
    data = []

    for cid in cid_list:
        smiles_url = f"https://pubchem.ncbi.nlm.nih.gov/rest/pug/compound/cid/{cid}/property/IsomericSMILES/txt"
        smiles_response = requests.get(smiles_url)
        smiles = smiles_response.text.strip()

        data.append({'CID': cid, 'IsomericSMILES': smiles})

    # Step 3: Save data to DataFrame and then to a CSV file
    df = pd.DataFrame(data)
    df.to_csv(f"output_{assay_id}.csv", index=False)
    display(df)

    # Step 4: Perform similarity search
    prolog = "https://pubchem.ncbi.nlm.nih.gov/rest/pug"
    cids_hit = dict()

    for idx, mysmiles in enumerate(df['IsomericSMILES']):
        mydata = {'smiles': mysmiles}
        url = prolog + "/compound/fastsimilarity_2d/smiles/cids/txt"
        res = requests.post(url, data=mydata)

        if res.status_code == 200:
            cids = res.text.split()
            cids = [int(x) for x in cids]    # Convert CIDs from string to integer.
        else:
            print("Error at", idx, ":", df.loc[idx, 'CID'], mysmiles)
            print(res.status_code)
            print(res.content)

        for mycid in cids:
            cids_hit[mycid] = cids_hit.get(mycid, 0) + 1

        time.sleep(0.2)

    # Step 5: Exclude query compounds from hits
    cids_query = dict()

    for idx, mysmiles in enumerate(df['IsomericSMILES']):
        mydata = {'smiles': mysmiles}
        url = prolog + "/compound/fastidentity/smiles/cids/txt?identity_type=same_connectivity"
        res = requests.post(url, data=mydata)

        if res.status_code == 200:
            cids = res.text.split()
            cids = [int(x) for x in cids]
        else:
            print("Error at", idx, ":", df.loc[idx, 'CID'], mysmiles)
            print(res.status_code)
            print(res.content)

        for mycid in cids:
            cids_query[mycid] = cids_query.get(mycid, 0) + 1

        time.sleep(0.2)

    # Step 6: Exclude query compounds from hits
    cids_hit = {k: v for k, v in cids_hit.items() if k not in cids_query}

    # Step 7: Filtering out non-drug-like compounds
    chunk_size = 100

    if len(cids_hit) % chunk_size == 0:
        num_chunks = len(cids_hit) // chunk_size
    else:
        num_chunks = len(cids_hit) // chunk_size + 1

    cids_list = list(cids_hit.keys())

    print("# Number of chunks:", num_chunks)

    csv = ""   # sets a variable called csv to save the comma-separated output

    for i in range(num_chunks):
        print(i, end=" ")

        idx1 = chunk_size * i
        idx2 = chunk_size * (i + 1)

        cids_str = ",".join([str(x) for x in cids_list[idx1:idx2]])  # build pug input for chunks of data
        url = prolog + f"/compound/cid/{cids_str}/property/HBondDonorCount,HBondAcceptorCount,MolecularWeight,XLogP,CanonicalSMILES,IsomericSMILES/csv"

        res = requests.get(url)

        if i == 0:  # if this is the first request, store result in an empty csv variable
            csv = res.text
        else:          # if this is a subsequent request, add the request to the csv variable adding a new line between chunks
            csv = csv + "\n".join(res.text.split()[1:]) + "\n"

        time.sleep(0.2)

    # Step 8: Downloaded data (in CSV) are loaded into a pandas data frame
    csv_file = StringIO(csv)
    df_raw = pd.read_csv(csv_file, sep=",")

    # Step 9: Show the shape (dimensions) of the data frame
    print("DataFrame Shape:", df_raw.shape)

    # Step 10: First load the cids_hit dictionary into a data frame
    df_freq = pd.DataFrame(cids_hit.items(), columns=['CID', 'HitFreq'])
    df_freq.head(5)

    # Step 11: Create a new data frame called "df" by joining the df and df_freq data frames
    df = df_raw.join(df_freq.set_index('CID'), on='CID')
    df.shape
    df.sort_values(by=['HitFreq', 'CID'], ascending=False).head(10)

    # Step 13: Draw the structures of the top 10 compounds
    cids_top = df.sort_values(by=['HitFreq', 'CID'], ascending=False).head(10).CID.to_list()
    # Save the original DataFrame to a CSV file
    df.to_csv(f'Predict candiate active chemical without filter no drug for {assay_id} bioassay AID.csv', index=False)
    fil = str(f'Predict candiate active chemical without filter no drug for {assay_id} bioassay AID.csv')
    print(" it is save your file ", fil)
    mols = []

    for mycid in cids_top:
        mysmiles = df[df.CID == mycid].IsomericSMILES.item()
        mol = Chem.MolFromSmiles(mysmiles)
        Chem.FindPotentialStereoBonds(mol)  # Identify potential stereo bonds!
        mols.append(mol)

    mylegends = ["CID " + str(x) for x in cids_top]
    img = Draw.MolsToGridImage(mols, molsPerRow=2, subImgSize=(400, 400), legends=mylegends)
    display(img)
    return
# Function to remove low-variance features from input data
from lazypredict.Supervised import LazyClassifier

def remove_low_variance_features(X):
    selection = VarianceThreshold(threshold=(.8 * (1 - .8)))
    X_filtered = selection.fit_transform(X)
    return X_filtered

# Function for lazy imports of specific modules
def lazy_imports():
    global train_test_split, LabelEncoder

def pubchem_bioact_desc_ml_classify(aid, output_file=None):
    # Function to convert AID to CIDs for both active and inactive compounds
    def aid_to_cids(aid):
        url_active = f"https://pubchem.ncbi.nlm.nih.gov/rest/pug/assay/aid/{aid}/cids/txt?cids_type=active"
        url_inactive = f"https://pubchem.ncbi.nlm.nih.gov/rest/pug/assay/aid/{aid}/cids/txt?cids_type=inactive"

        response_active = requests.get(url_active)
        response_inactive = requests.get(url_inactive)

        cids_active = response_active.text.split() if response_active.status_code == 200 else []
        cids_inactive = response_inactive.text.split() if response_inactive.status_code == 200 else []

        return cids_active, cids_inactive

    # Function to convert CID to Isomeric SMILES
    def cid_to_smiles(cid):
        url = f"https://pubchem.ncbi.nlm.nih.gov/rest/pug/compound/cid/{cid}/property/IsomericSMILES/txt"
        response = requests.get(url)
        if response.status_code == 200:
            smiles = response.text.strip()
            return smiles
        else:
            print(f"Error: Unable to retrieve Isomeric SMILES for CID {cid}")
            return None

    # Convert AID to CIDs for both active and inactive compounds
    cids_active, cids_inactive = aid_to_cids(aid)

    # Initialize lists to store data
    data = []

    # Convert each CID to Isomeric SMILES for active compounds
    for cid in cids_active:
        isomeric_smiles = cid_to_smiles(cid)
        if isomeric_smiles:
            data.append((aid, cid, isomeric_smiles, 'active'))
        else:
            print(f"Conversion failed for CID {cid}")

    # Convert each CID to Isomeric SMILES for inactive compounds
    for cid in cids_inactive:
        isomeric_smiles = cid_to_smiles(cid)
        if isomeric_smiles:
            data.append((aid, cid, isomeric_smiles, 'inactive'))
        else:
            print(f"Conversion failed for CID {cid}")

    # Create a DataFrame
    df = pd.DataFrame(data, columns=['AID', 'CID', 'IsomericSMILES', 'Status'])
    display(df)

    # Generate molecular descriptors using Mordred
    smiles_list = df['IsomericSMILES'].tolist()
    calc = Calculator(descriptors)
    descriptor_dicts = []

    for smiles in smiles_list:
        mol = Chem.MolFromSmiles(smiles)
        if mol:
            descriptors_values = calc(mol)
            result_dict = dict(zip(descriptors_values.keys(), descriptors_values))
            descriptor_dicts.append(result_dict)
        else:
            print(f"Invalid SMILES: {smiles}")

    # Convert the list of dictionaries to a DataFrame
    descriptor_df = pd.DataFrame(descriptor_dicts)

    # Merge the original DataFrame with the descriptor DataFrame
    result_df = pd.concat([df, descriptor_df], axis=1)

    # Display the DataFrame
    df = result_df
    display(df)

    # Check if 'Status' column is present before encoding
    if 'Status' in df.columns:
        # Encode the 'Status' column to binary values (1 for active, 0 for inactive)
        label_encoder = LabelEncoder()
        df['Status'] = label_encoder.fit_transform(df['Status'])
        # Convert 'CID' column to integers
        df['CID'] = pd.to_numeric(df['CID'], errors='coerce').astype('Int64')

        # Drop columns with string values
        string_columns = df.select_dtypes(include=['object']).columns
        df = df.drop(columns=string_columns)

        # Features are the molecular descriptors, target is the 'Status' column
        X = df.drop(['AID', 'CID', 'Status'], axis=1)
        display(X)
        y = df['Status']
        display(y)
        # Split the data into training and testing sets

        # Get target values meanings
        target_meanings = get_target_values_meanings(y)

    # Train-test split
        X_train, X_test, y_train, y_test = train_test_split(X,y, test_size=0.2, random_state=2)

        # LazyClassifier
        clf = LazyClassifier(verbose=0, ignore_warnings=True, custom_metric=None, predictions=True)
        models, _ = clf.fit(X_train, X_test, y_train, y_test)

        # Plot the model accuracies
        plt.figure(figsize=(5, 10))
        sns.set_theme(style="whitegrid")
        ax = sns.barplot(y=models.index, x="Accuracy", data=models, palette='viridis')
        ax.set(xlim=(0, 1))
        plt.show()

        # RandomForestClassifier
        clf_rf = RandomForestClassifier(n_estimators=500, random_state=1)
        clf_rf.fit(X_train, y_train)
        y_pred_class_rf = clf_rf.predict(X_test)

        # XGBClassifier
        xgbc = XGBClassifier()
        xgbc.fit(X_train, y_train)
        y_pred_class_xgb = xgbc.predict(X_test)

        # Cross-validation scores
        scores_rf = cross_val_score(clf_rf, X_train, y_train, cv=5)
        print("Random Forest Mean cross-validation score: %.2f" % scores_rf.mean())

        kfold = KFold(n_splits=10, shuffle=True)
        kf_cv_scores_xgb = cross_val_score(xgbc, X_train, y_train, cv=kfold)
        print("XGBClassifier K-fold CV average score: %.2f" % kf_cv_scores_xgb.mean())

        # Feature Importance with Random Forest
        importance_rf = clf_rf.feature_importances_
        feature_names = X.columns
        fp_rf = sorted(range(len(importance_rf)), key=lambda i: importance_rf[i], reverse=True)[:20]
        imp_values_rf = sorted(importance_rf, reverse=True)[:20]

        feature_names_rf = [feature_names[i] for i in fp_rf]
        imp_values_rf

        fake_rf = pd.DataFrame({'ind': feature_names_rf, 'importance__': imp_values_rf})


        clf = RandomForestClassifier(n_estimators=500, random_state=1)
        clf.fit(X_train, y_train)
        y_pred_class = clf.predict(X_test)

        cf_matrix = confusion_matrix(y_test, y_pred_class)

        # Plot confusion matrix with target values meanings as titles
        sns.heatmap(cf_matrix, annot=True, cmap='Blues', xticklabels=list(target_meanings.values()), yticklabels=list(target_meanings.values()))
        plt.xlabel('Predicted')
        plt.ylabel('True')
        plt.title('Confusion Matrix')
        plt.show()

        print(classification_report(y_test, y_pred_class))
        return

#pubchem_bioact_desc_ml_classify(1000)

def perform_classification(csv_file_path):
    # Open and display the CSV file
    df = pd.read_csv(csv_file_path)

    # Encode the 'Status' column if needed
    if 'Status' in df.columns:
        le = LabelEncoder()
        df['Status'] = le.fit_transform(df['Status'])

    # Drop columns with object data type
    df = df.select_dtypes(exclude=['object'])

    # Split the data into training and testing sets
    X = df.drop(['Status'], axis=1)
    y = df['Status']

    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

    # Initialize and train the classifier
    classifier = RandomForestClassifier()
    classifier.fit(X_train, y_train)

    # Make predictions
    predictions = classifier.predict(X_test)

    # Evaluate accuracy
    accuracy = accuracy_score(y_test, predictions)
    print(f'Accuracy: {accuracy}')

    # Classification Report
    report_dict = classification_report(y_test, predictions, output_dict=True)
    report_df = pd.DataFrame(report_dict).transpose()

    # Save the classification report to a CSV file
    report_df.to_csv("classification_report.csv", index=False)
    display(report_df)
# Example usage
#csv_file_path = "Machine_learning_for_bioassay_1000.csv"
#perform_classification(csv_file_path)



def PCFP_BitString(pcfp_base64):
    pcfp_bitstring = "".join(["{:08b}".format(x) for x in b64decode(pcfp_base64)])[32:913]
    return pcfp_bitstring

def get_target_values_meanings(y):
    unique_values = y.unique()
    meanings = {}
    for val in unique_values:
        meaning = input(f"Enter the meaning of '{val}': ")
        meanings[val] = meaning
    return meanings

def qsar_assay_fingerprint_analysis(aid_input):
    def retrieve_active_inactive_cids_with_fingerprints_and_pcfp(aid):
        url = f'https://pubchem.ncbi.nlm.nih.gov/rest/pug/assay/aid/{aid}/cids/txt?cids_type=active'

        # Fetch active CIDs
        response = requests.get(url)
        active_cids = set(map(int, response.text.strip().split()))

        # Create DataFrame
        df1 = pd.DataFrame(list(active_cids), columns=['CID'])
        df1['Activity'] = 'Active'
        url = f'https://pubchem.ncbi.nlm.nih.gov/rest/pug/assay/aid/{aid}/cids/txt?cids_type=inactive'

        # Fetch active CIDs
        response = requests.get(url)
        active_cids = set(map(int, response.text.strip().split()))

        # Create DataFrame
        df2 = pd.DataFrame(list(active_cids), columns=['CID'])
        df2['Activity'] = 'Inactive'
        # Combine df1 and df2 into a single DataFrame
        df = pd.concat([df1, df2], ignore_index=True)
        display(df)

        # Retrieve fingerprints and PCFP for each CID
        for cid in df['CID']:
            fingerprint_url = f'https://pubchem.ncbi.nlm.nih.gov/rest/pug/compound/cid/{cid}/property/Fingerprint2D/txt'
            fingerprint_response = requests.get(fingerprint_url)
            fingerprint = fingerprint_response.text.strip()

            # Add fingerprint to the DataFrame
            df.loc[df['CID'] == cid, 'Fingerprint2D'] = fingerprint

            # Decode base64 fingerprint and extract PCFP bitstring
            pcfp_bitstring = PCFP_BitString(fingerprint)

            # Add PCFP bitstring columns to the DataFrame
            pcfp_columns = [f'PubchemFP{i}' for i in range(len(pcfp_bitstring))]
            df.loc[df['CID'] == cid, pcfp_columns] = list(pcfp_bitstring)

        return df

    result_df = retrieve_active_inactive_cids_with_fingerprints_and_pcfp(aid_input)
    result_df = result_df.drop(['CID', 'Fingerprint2D'], axis=1)
    # Encode the 'Activity' column
    le_activity = LabelEncoder()
    result_df['Activity'] = le_activity.fit_transform(result_df['Activity'])
    result_df = result_df.astype(int)
    display(result_df)
    X=result_df.drop('Activity', axis=1)
    y=result_df['Activity']
    # Get target values meanings
    target_meanings = get_target_values_meanings(y)

 # Train-test split
    X_train, X_test, y_train, y_test = train_test_split(result_df.drop('Activity', axis=1),result_df['Activity'], test_size=0.2, random_state=2)

    # LazyClassifier
    clf = LazyClassifier(verbose=0, ignore_warnings=True, custom_metric=None, predictions=True)
    models, _ = clf.fit(X_train, X_test, y_train, y_test)

    # Plot the model accuracies
    plt.figure(figsize=(5, 10))
    sns.set_theme(style="whitegrid")
    ax = sns.barplot(y=models.index, x="Accuracy", data=models, palette='viridis')
    ax.set(xlim=(0, 1))
    plt.show()

    # RandomForestClassifier
    clf_rf = RandomForestClassifier(n_estimators=500, random_state=1)
    clf_rf.fit(X_train, y_train)
    y_pred_class_rf = clf_rf.predict(X_test)

    # XGBClassifier
    xgbc = XGBClassifier()
    xgbc.fit(X_train, y_train)
    y_pred_class_xgb = xgbc.predict(X_test)

    # Cross-validation scores
    scores_rf = cross_val_score(clf_rf, X_train, y_train, cv=5)
    print("Random Forest Mean cross-validation score: %.2f" % scores_rf.mean())

    kfold = KFold(n_splits=10, shuffle=True)
    kf_cv_scores_xgb = cross_val_score(xgbc, X_train, y_train, cv=kfold)
    print("XGBClassifier K-fold CV average score: %.2f" % kf_cv_scores_xgb.mean())

    # Feature Importance with Random Forest
    importance_rf = clf_rf.feature_importances_
    feature_names = X.columns
    fp_rf = sorted(range(len(importance_rf)), key=lambda i: importance_rf[i], reverse=True)[:20]
    imp_values_rf = sorted(importance_rf, reverse=True)[:20]

    feature_names_rf = [feature_names[i] for i in fp_rf]
    imp_values_rf

    fake_rf = pd.DataFrame({'ind': feature_names_rf, 'importance__': imp_values_rf})

    # Plot Feature Importance
    sns.set_color_codes("pastel")
    ax_rf = sns.barplot(x='ind', y='importance__', data=fake_rf)
    ax_rf.set(xlabel='Features', ylabel='importance')
    plt.xticks(rotation=90)
    plt.tight_layout()
    plt.show()

    clf = RandomForestClassifier(n_estimators=500, random_state=1)
    clf.fit(X_train, y_train)
    y_pred_class = clf.predict(X_test)

    cf_matrix = confusion_matrix(y_test, y_pred_class)

    # Plot confusion matrix with target values meanings as titles
    sns.heatmap(cf_matrix, annot=True, cmap='Blues', xticklabels=list(target_meanings.values()), yticklabels=list(target_meanings.values()))
    plt.xlabel('Predicted')
    plt.ylabel('True')
    plt.title('Confusion Matrix')
    plt.show()

    print(classification_report(y_test, y_pred_class))
    return



    #aid_input = input("Enter the AID: ")


def perform_classification(csv_file_path):
    # Open and display the CSV file
    df = pd.read_csv(csv_file_path)

    # Encode the 'Status' column if needed
    if 'Status' in df.columns:
        le = LabelEncoder()
        df['Status'] = le.fit_transform(df['Status'])

    # Drop columns with object data type
    df = df.select_dtypes(exclude=['object'])

    # Split the data into training and testing sets
    X = df.drop(['Status'], axis=1)
    y = df['Status']

    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

    # Initialize and train the classifier
    classifier = RandomForestClassifier()
    classifier.fit(X_train, y_train)

    # Make predictions
    predictions = classifier.predict(X_test)

    # Evaluate accuracy
    accuracy = accuracy_score(y_test, predictions)
    print(f'Accuracy: {accuracy}')

    # Classification Report
    report_dict = classification_report(y_test, predictions, output_dict=True)
    report_df = pd.DataFrame(report_dict).transpose()

    # Save the classification report to a CSV file
    report_df.to_csv("classification_report.csv", index=False)
    display(report_df)
# Example usage
#csv_file_path = "Machine_learning_for_bioassay_1000.csv"
#perform_classification(csv_file_path)



def PCFP_BitString(pcfp_base64):
    pcfp_bitstring = "".join(["{:08b}".format(x) for x in b64decode(pcfp_base64)])[32:913]
    return pcfp_bitstring

    #aid_input = input("Enter the AID: ")
#qsar_assay_fingerprint_analysis(1000)
def virtual_screening(file_path, *colnames):
    # Convert variable number of column names to a list
    col_names = list(colnames)

    # Read the input file into a pandas DataFrame
    df_act = pd.read_csv(file_path, sep=" ", names=col_names)
    smiles_act = df_act['smiles'].tolist()

    prolog = "https://pubchem.ncbi.nlm.nih.gov/rest/pug"
    cids_hit = {}

    # Loop through the input smiles and perform similarity search against PubChem
    for idx, mysmiles in enumerate(smiles_act):
        mydata = {'smiles': mysmiles}

        # Similarity Search against PubChem
        url = f"{prolog}/compound/fastsimilarity_2d/smiles/cids/txt"
        res = requests.post(url, data=mydata)

        if res.status_code == 200:
            cids = [int(x) for x in res.text.split()]
        else:
            # Handle errors during the PubChem request
            print(f"Error at {idx}: {df_act.loc[idx, 'id']} {mysmiles}")
            print(res.status_code)
            print(res.content)

        # Update the dictionary with hit counts for each CID
        for mycid in cids:
            cids_hit[mycid] = cids_hit.get(mycid, 0) + 1
        time.sleep(0.2)

    # Exclude the query compounds from the hits
    cids_query = {}
    for idx, mysmiles in enumerate(smiles_act):
        mydata = {'smiles': mysmiles}
        url = f"{prolog}/compound/fastidentity/smiles/cids/txt?identity_type=same_connectivity"
        res = requests.post(url, data=mydata)

        if res.status_code == 200:
            cids = [int(x) for x in res.text.split()]
        else:
            # Handle errors during the PubChem request
            print(f"Error at {idx}: {df_act.loc[idx, 'id']} {mysmiles}")
            print(res.status_code)
            print(res.content)

        # Update the dictionary with query CID counts
        for mycid in cids:
            cids_query[mycid] = cids_query.get(mycid, 0) + 1
        time.sleep(0.2)

    # Remove query compounds from the hit dictionary
    for mycid in cids_query.keys():
        cids_hit.pop(mycid, None)

    # Filtering out non-drug-like compounds
    chunk_size = 100
    if len(cids_hit) % chunk_size == 0:
        num_chunks = len(cids_hit) // chunk_size
    else:
        num_chunks = len(cids_hit) // chunk_size + 1

    cids_list = list(cids_hit.keys())
    csv = ""

    # Loop through chunks and retrieve additional properties from PubChem
    for i in range(num_chunks):
        idx1 = chunk_size * i
        idx2 = chunk_size * (i + 1)
        cids_str = ",".join(map(str, cids_list[idx1:idx2]))
        url = f"{prolog}/compound/cid/{cids_str}/property/HBondDonorCount,HBondAcceptorCount,MolecularWeight,XLogP,CanonicalSMILES,IsomericSMILES/csv"
        res = requests.get(url)

        # Append results to CSV variable
        if i == 0:
            csv = res.text
        else:
            csv += "\n".join(res.text.split()[1:]) + "\n"

        time.sleep(0.2)

    # Downloaded data (in CSV) are loaded into a pandas data frame
    csv_file = StringIO(csv)
    df_raw = pd.read_csv(csv_file, sep=",")

    # Lipinski's rule of five criteria
    df = df_raw[(df_raw['HBondDonorCount'] <= 5) &
                (df_raw['HBondAcceptorCount'] <= 10) &
                (df_raw['MolecularWeight'] <= 500) &
                (df_raw['XLogP'] < 5)]

    # Draw the structures of the top 10 compounds
    print("The top 10 unique compounds that satisfy all criteria of Lipinski's rule of five")
    cids_top = df.sort_values(by=['HitFreq', 'CID'], ascending=False).head(10).CID.tolist()

    mols = []
    for mycid in cids_top:
        mysmiles = df[df.CID == mycid].IsomericSMILES.item()
        mol = Chem.MolFromSmiles(mysmiles)
        Chem.FindPotentialStereoBonds(mol)
        mols.append(mol)

    mylegends = [f"CID {x}" for x in cids_top]
    img = Draw.MolsToGridImage(mols, molsPerRow=2, subImgSize=(400, 400), legends=mylegends)
    display(img)

    # Extract unique compounds in terms of canonical SMILES
    canonical_smiles = df.CanonicalSMILES.unique()
    idx_to_include = []

    for mysmiles in canonical_smiles:
        myidx = df[df.CanonicalSMILES == mysmiles].index.to_list()[0]
        idx_to_include.append(myidx)

    df['Include'] = 0
    df.loc[idx_to_include, 'Include'] = 1

    cids_top_unique = df[df['Include'] == 1].sort_values(by=['HitFreq', 'CID'], ascending=False).head(10).CID.tolist()

    # Draw the top 10 unique compounds
    print("The top 10 unique compounds in terms of canonical SMILES")
    mols = []

    for mycid in cids_top_unique:
        mysmiles = df[df.CID == mycid].IsomericSMILES.item()
        mol = Chem.MolFromSmiles(mysmiles)
        Chem.FindPotentialStereoBonds(mol)
        mols.append(mol)

    mylegends = [f"CID {x}" for x in cids_top_unique]
    img = Draw.MolsToGridImage(mols, molsPerRow=2, subImgSize=(400, 400), legends=mylegends)
    display(img)

    # Saving molecules in files
    for idx, mycid in enumerate(cids_top_unique):
        if idx == 3:
            break

        mysmiles = df[df['CID'] == mycid].IsomericSMILES.item()
        mymol = Chem.MolFromSmiles(mysmiles)
        mymol = Chem.AddHs(mymol)
        AllChem.EmbedMolecule(mymol)
        AllChem.MMFFOptimizeMolecule(mymol)

        filename = f"{file_path}_lig{idx}_{mycid}.mol"
        Chem.MolToMolFile(mymol, filename)

    df.to_csv('Virtual_Screening.csv', index=False)
    print("Now, you can find 'Virtual_Screening.csv' in your device")

    return df.to_csv('Virtual_Screening.csv')


import pandas as pd
from rdkit import Chem
from rdkit.Chem import Draw
from rdkit.Chem import rdFMCS

def read_csv_and_find_common_substructure(csv_file_path):
    """
    Reads a CSV file containing SMILES representations of compounds and finds the common substructure.

    Parameters:
    - csv_file_path (str): Path to the CSV file containing SMILES representations.

    Returns:
    - None
    """

    # Read CSV file
    df = pd.read_csv(csv_file_path)
    display(df)

    # Extract SMILES from the DataFrame
    smiles_list = df['PUBCHEM_EXT_DATASOURCE_SMILES'].tolist()

    # Convert SMILES to RDKit molecules
    # Convert SMILES to RDKit molecules, skipping empty cells
    # Convert valid SMILES to RDKit molecules
    molecules = []
    for smiles in smiles_list:
        if isinstance(smiles, str) and smiles.strip():  # Check if it's a non-empty string
            mol = Chem.MolFromSmiles(smiles)
            if mol:
                molecules.append(mol)

    # Draw molecules
    grid_image = Draw.MolsToGridImage(molecules, molsPerRow=5, subImgSize=(400, 400))
    display(grid_image)

    # Find common substructure
    mcs_result = rdFMCS.FindMCS(molecules, threshold=0.7)
    common_substructure = Chem.MolFromSmarts(mcs_result.smartsString)

    print("The common substructure for these compounds:")
    display(common_substructure)

# Example usage:
#read_csv_and_find_common_substructure("AID_1860_datatable_inactive.csv")
import pandas as pd
from rdkit import Chem

def visualize_molecules_from_smiles(file_path, smiles_column):
    """
    Visualize molecules from SMILES strings in a CSV file.

    Parameters:
        file_path (str): Path to the CSV file containing molecular data.
        smiles_column (str): Name of the column containing SMILES strings.

    Returns:
        None
    """
    # Read CSV file
    df = pd.read_csv(file_path)

    # Extract SMILES from the DataFrame
    smiles_list = df[smiles_column].tolist()

    # Convert SMILES to RDKit molecules, skipping empty cells
    molecules = [Chem.MolFromSmiles(smiles) for smiles in smiles_list if isinstance(smiles, str) and smiles.strip()]

    # Draw molecules
    grid_image = Draw.MolsToGridImage(molecules, molsPerRow=5, subImgSize=(400, 400))
    display(grid_image)

# Example Usage:
#file_path = "dataset_ise.csv"
#visualize_molecules_from_smiles(file_path, "smiles")

def virtual_screening_2(csv_file, smiles_column):
    # Read CSV file
    df = pd.read_csv(csv_file, header=None)
    df_act = df.copy()

    # Display DataFrame
    print("DataFrame:")
    display(df)

    # Extract SMILES strings
    smiles_act = df[smiles_column].tolist()

    # Print number of structures in DataFrame
    print("Number of structures in the DataFrame:", len(df))

    prolog = "https://pubchem.ncbi.nlm.nih.gov/rest/pug"
    cids_hit = dict()

    for idx, mysmiles in enumerate(smiles_act):
        mydata = {'smiles': mysmiles}
        url = f"{prolog}/compound/fastsimilarity_2d/smiles/cids/txt"
        print("Request URL:", url)  # Print request URL

        res = requests.post(url, data=mydata)

        # Handle response status code
        if res.status_code == 200:
            cids = res.text.split()
            cids = [int(x) for x in cids]  # Convert CIDs from string to integer.
        else:
            # Print error information
            print("Error at index", idx, "with SMILES:", mysmiles)
            print("Status code:", res.status_code)
            print("Content:", res.content)
            continue

        # Update cids_hit dictionary
        for mycid in cids:
            cids_hit[mycid] = cids_hit.get(mycid, 0) + 1

        time.sleep(0.2)

    # Sort the dictionary by frequency
    sorted_by_freq = [(v, k) for k, v in cids_hit.items()]
    sorted_by_freq.sort(reverse=True)

    # Print the top 10 results
    print("Top 10 results (frequency, CID):")
    for v, k in enumerate(sorted_by_freq[:10]):
        print(v, k)  # Print (frequency, CID)

    cids_query = dict()

    for idx, mysmiles in enumerate(smiles_act):

        mydata = {'smiles': mysmiles}
        url = prolog + "/compound/fastidentity/smiles/cids/txt?identity_type=same_connectivity"
        res = requests.post(url, data=mydata)

        if (res.status_code == 200):
            cids = res.text.split()
            cids = [int(x) for x in cids]
        else:
            print("Error at", idx, ":", df_act.loc[idx, smiles_column], mysmiles)  # Using smiles_column instead of k
            print(res.status_code)
            print(res.content)

        for mycid in cids:
            cids_query[mycid] = cids_query.get(mycid, 0) + 1

        time.sleep(0.2)
        sorted_by_freq = [(v, k) for k, v in cids_hit.items()]
        sorted_by_freq.sort(reverse=True)

        for v, k in enumerate(sorted_by_freq):

            if v == 10:
                break

            print(v, k)  # Print (frequency, CID)
    sorted_by_freq = [(v, k) for k, v in cids_hit.items()]
    sorted_by_freq.sort(reverse=True)

    for v, k in enumerate(sorted_by_freq):

        if v == 10:
            break

        print(v, k)  # Print (frequency, CID)

    cids_query = dict()

    for idx, mysmiles in enumerate(smiles_act):

        mydata = {'smiles': mysmiles}
        url = prolog + "/compound/fastidentity/smiles/cids/txt?identity_type=same_connectivity"
        res = requests.post(url, data=mydata)

        if (res.status_code == 200):
            cids = res.text.split()
            cids = [int(x) for x in cids]
        else:
            print("Error at", idx, ":", df_act.loc[idx, smiles_column], mysmiles)  # Using smiles_column instead of k
            print(res.status_code)
            print(res.content)

        for mycid in cids:
            cids_query[mycid] = cids_query.get(mycid, 0) + 1

        time.sleep(0.2)

    len(cids_query.keys())  # Show the number of CIDs that represent the query compounds.
    for mycid in cids_query.keys():
        cids_hit.pop(mycid, None)
    len(cids_hit)

    sorted_by_freq = [(v, k) for k, v in cids_hit.items()]
    sorted_by_freq.sort(reverse=True)

    for v, k in enumerate(sorted_by_freq):

        if v == 10:
            break

        print(v, k)  # Print (frequency, CID)
    sorted_by_freq = [(v, k) for k, v in cids_hit.items()]
    sorted_by_freq.sort(reverse=True)

    for v, k in enumerate(sorted_by_freq):

        if v == 10:
            break

        print(v, k)  # Print (frequency, CID)

    chunk_size = 100

    if (len(cids_hit) % chunk_size == 0):
        num_chunks = len(cids_hit) // chunk_size
    else:
        num_chunks = len(cids_hit) // chunk_size + 1

    cids_list = list(cids_hit.keys())

    print("# Number of chunks:", num_chunks)

    csv = ""  # sets a variable called csv to save the comma separated output

    for i in range(num_chunks):

        print(i, end=" ")

        idx1 = chunk_size * i
        idx2 = chunk_size * (i + 1)

        cids_str = ",".join([str(x) for x in cids_list[idx1:idx2]])  # build pug input for chunks of data
        url = prolog + "/compound/cid/" + cids_str + "/property/HBondDonorCount,HBondAcceptorCount,MolecularWeight,XLogP,CanonicalSMILES,IsomericSMILES/csv"

        res = requests.get(url)

        if (i == 0):  # if this is the first request, store result in empty csv variable
            csv = res.text
        else:  # if this is a subsequent request, add the request to the csv variable adding a new line between chunks
            csv = csv + "\n".join(res.text.split()[1:]) + "\n"

        time.sleep(0.2)

    # print(csv)
    from io import StringIO

    csv_file = StringIO(csv)

    df_raw = pd.read_csv(csv_file, sep=",")

    df_raw.shape  # Show the shape (dimesnion) of the data frame
    df_raw.isna().sum()  # Check if there are any NULL values.
    len(df_raw)  # Check the number of rows (which is equals to the number of CIDs)
    # First load the cids_hit dictionary into a data frame.
    df_freq = pd.DataFrame(cids_hit.items(), columns=['CID', 'HitFreq'])
    df_freq.head(5)
    # Double-check if the data are loaded correctly
    # Compare the data with those from Cell [12]
    df_freq.sort_values(by=['HitFreq', 'CID'], ascending=False).head(10)
    # Create a new data frame called "df" by joining the df and df_freq data frames
    df = df_raw.join(df_freq.set_index('CID'), on='CID')
    df.shape
    df.sort_values(by=['HitFreq', 'CID'], ascending=False).head(10)
    df = df[(df['HBondDonorCount'] <= 5) &
            (df['HBondAcceptorCount'] <= 10) &
            (df['MolecularWeight'] <= 500) &
            (df['XLogP'] < 5)]
    # Draw the structures of the top 10 compounds

    cids_top = df.sort_values(by=['HitFreq', 'CID'], ascending=False).head(10).CID.to_list()
    mols = []

    for mycid in cids_top:
        mysmiles = df[df.CID == mycid].IsomericSMILES.item()

        mol = Chem.MolFromSmiles(mysmiles)
        Chem.FindPotentialStereoBonds(mol)  # Identify potential stereo bonds!
        mols.append(mol)

    mylegends = ["CID " + str(x) for x in cids_top]
    img = Draw.MolsToGridImage(mols, molsPerRow=2, subImgSize=(400, 400), legends=mylegends)
    display(img)

    # Extract unique compounds in terms of canonical SMILES

    canonical_smiles = df.CanonicalSMILES.unique()
    idx_to_include = []

    for mysmiles in canonical_smiles:
        myidx = df[df.CanonicalSMILES == mysmiles].index.to_list()[0]

        idx_to_include.append(myidx)
    # Create a new column 'Include'
    # All values initialized to 0 (not include)
    df['Include'] = 0
    df['Include'].sum()
    # Now the "Include" column's value is modified if the record is in the idx_to_include list.
    df.loc[idx_to_include, 'Include'] = 1
    df['Include'].sum()
    # Now draw the top 10 unique compounds (in terms of canonical SMILES)
    cids_top = df[df['Include'] == 1].sort_values(by=['HitFreq', 'CID'], ascending=False).head(10).CID.to_list()
    mols = []

    for mycid in cids_top:
        mysmiles = df[df.CID == mycid].IsomericSMILES.item()

        mol = Chem.MolFromSmiles(mysmiles)
        Chem.FindPotentialStereoBonds(mol)  # Identify potential stereo bonds!
        mols.append(mol)

    mylegends = ["CID " + str(x) for x in cids_top]
    img = Draw.MolsToGridImage(mols, molsPerRow=2, subImgSize=(400, 400), legends=mylegends)
    display(img)

    from rdkit.Chem import AllChem

    for idx, mycid in enumerate(cids_top):

        if idx == 3:
            break

        mysmiles = df[df['CID'] == mycid].IsomericSMILES.item()

        mymol = Chem.MolFromSmiles(mysmiles)
        mymol = Chem.AddHs(mymol)
        AllChem.EmbedMolecule(mymol)
        AllChem.MMFFOptimizeMolecule(mymol)

        filename = "screenig result" + str(idx) + "_" + str(mycid) + ".mol"
        Chem.MolToMolFile(mymol, filename)
    df.to_csv('screening result.csv')


# Example usage:
#csv_file = "AID_1107225_datatable_active.csv"  # Path to your CSV file
#smiles_column = 3  # Index of the column containing SMILES strings (assuming it's the fourth column)
#virtual_screening_2(csv_file, smiles_column)
from rdkit.Chem import AllChem  # Add this import statement

def virtual_screening_3(csv_file, smiles_column):
    # Read CSV file
    df = pd.read_csv(csv_file)
    df_act = df.copy()

    # Display DataFrame
    print("DataFrame:")
    display(df)

    # Extract SMILES strings
    smiles_act = df[smiles_column].tolist()

    # Print number of structures in DataFrame
    print("Number of structures in the DataFrame:", len(df))

    prolog = "https://pubchem.ncbi.nlm.nih.gov/rest/pug"
    cids_hit = dict()

    # Perform similarity search
    for idx, mysmiles in enumerate(smiles_act):
        mydata = {'smiles': mysmiles}
        url = f"{prolog}/compound/fastsimilarity_2d/smiles/cids/txt"
        print("Request URL:", url)  # Print request URL

        res = requests.post(url, data=mydata)

        # Handle response status code
        if res.status_code == 200:
            cids = res.text.split()
            cids = [int(x) for x in cids]  # Convert CIDs from string to integer.
        else:
            # Print error information
            print("Error at index", idx, "with SMILES:", mysmiles)
            print("Status code:", res.status_code)
            print("Content:", res.content)
            continue

        # Update cids_hit dictionary
        for mycid in cids:
            cids_hit[mycid] = cids_hit.get(mycid, 0) + 1

        time.sleep(0.2)

    # Sort the dictionary by frequency
    sorted_by_freq = [(v, k) for k, v in cids_hit.items()]
    sorted_by_freq.sort(reverse=True)

    # Print the top 10 results of similarity search
    print("Top 10 results of similarity search (frequency, CID):")
    for v, k in enumerate(sorted_by_freq[:10]):
        print(v, k)  # Print (frequency, CID)

    cids_query = dict()

    # Perform query using fast identity search
    for idx, mysmiles in enumerate(smiles_act):
        mydata = {'smiles': mysmiles}
        url = prolog + "/compound/fastidentity/smiles/cids/txt?identity_type=same_connectivity"
        res = requests.post(url, data=mydata)

        if res.status_code == 200:
            cids = res.text.split()
            cids = [int(x) for x in cids]
        else:
            print("Error at", idx, ":", df_act.loc[idx, smiles_column], mysmiles)
            print(res.status_code)
            print(res.content)

        for mycid in cids:
            cids_query[mycid] = cids_query.get(mycid, 0) + 1

        time.sleep(0.2)

    # Sort and print the top 10 results of the query
    sorted_by_freq = [(v, k) for k, v in cids_hit.items()]
    sorted_by_freq.sort(reverse=True)
    print("Top 10 results of query (frequency, CID):")
    for v, k in enumerate(sorted_by_freq[:10]):
        print(v, k)  # Print (frequency, CID)
# Process remaining operations...

    # Calculate the number of chunks for API requests
    chunk_size = 100
    if len(cids_hit) % chunk_size == 0:
        num_chunks = len(cids_hit) // chunk_size
    else:
        num_chunks = len(cids_hit) // chunk_size + 1

    cids_list = list(cids_hit.keys())

    print("# Number of chunks:", num_chunks )

    csv = ""   # Initialize a variable to save the comma-separated output

    # Perform API requests in chunks
    for i in range(num_chunks):

        print(i, end=" ")

        idx1 = chunk_size * i
        idx2 = chunk_size * (i + 1)

        cids_str = ",".join([str(x) for x in cids_list[idx1:idx2]])  # Build PUG input for chunks of data
        url = prolog + f"/compound/cid/{cids_str}/property/HBondDonorCount,HBondAcceptorCount,MolecularWeight,XLogP,CanonicalSMILES,IsomericSMILES/csv"

        res = requests.get(url)

        if i == 0:  # If this is the first request, store result in empty csv variable
            csv = res.text
        else:       # If this is a subsequent request, add the request to the csv variable adding a new line between chunks
            csv = csv + "\n".join(res.text.split()[1:]) + "\n"

        time.sleep(0.2)

    # Read CSV file from string
    from io import StringIO
    csv_file = StringIO(csv)
    df_raw = pd.read_csv(csv_file, sep=",")

    # Further operations on df_raw
    df_raw_shape = df_raw.shape    # Show the shape (dimension) of the data frame
    df_raw_na_sum = df_raw.isna().sum()    # Check if there are any NULL values.
    df_raw_len = len(df_raw)    # Check the number of rows (which is equals to the number of CIDs)

    # Load the cids_hit dictionary into a data frame
    df_freq = pd.DataFrame(cids_hit.items(), columns=['CID','HitFreq'])
    df_freq_head = df_freq.head(5)  # Display first 5 rows
    df_freq_sorted = df_freq.sort_values(by=['HitFreq', 'CID'], ascending=False).head(10)  # Sort and display top 10 rows

    # Create a new data frame called "df" by joining the df_raw and df_freq data frames
    df = df_raw.join(df_freq.set_index('CID'), on='CID')
    df_shape = df.shape
    df_top_sorted = df.sort_values(by=['HitFreq', 'CID'], ascending=False).head(10)

    # Filter df based on conditions
    df_filtered = df[(df['HBondDonorCount'] <= 5) &
                     (df['HBondAcceptorCount'] <= 10) &
                     (df['MolecularWeight'] <= 500) &
                     (df['XLogP'] < 5)]

    # Draw the structures of the top 10 compounds
    cids_top = df_filtered.sort_values(by=['HitFreq', 'CID'], ascending=False).head(10).CID.to_list()
    mols = []

    for mycid in cids_top:
        mysmiles = df[df.CID == mycid].IsomericSMILES.item()
        mol = Chem.MolFromSmiles(mysmiles)
        Chem.FindPotentialStereoBonds(mol)    # Identify potential stereo bonds!
        mols.append(mol)

    mylegends = ["CID " + str(x) for x in cids_top]
    img = Draw.MolsToGridImage(mols, molsPerRow=2, subImgSize=(400,400), legends=mylegends)
    display(img)

    # Extract unique compounds in terms of canonical SMILES
    canonical_smiles = df_filtered.CanonicalSMILES.unique()
    idx_to_include = []

    for mysmiles in canonical_smiles:
        myidx = df_filtered[df_filtered.CanonicalSMILES == mysmiles].index.to_list()[0]
        idx_to_include.append(myidx)

    # Create a new column 'Include'
    # All values initialized to 0 (not include)
    df_filtered['Include'] = 0
    df_filtered.loc[idx_to_include, 'Include'] = 1  # Set 'Include' column's value to 1 if the record is in the idx_to_include list

    # Draw the top 10 unique compounds (in terms of canonical SMILES)
    cids_top_unique = df_filtered[df_filtered['Include'] == 1].sort_values(by=['HitFreq', 'CID'], ascending=False).head(10).CID.to_list()
    mols = []

    for mycid in cids_top_unique:
        mysmiles = df_filtered[df_filtered.CID == mycid].IsomericSMILES.item()
        mol = Chem.MolFromSmiles(mysmiles)
        Chem.FindPotentialStereoBonds(mol)    # Identify potential stereo bonds!
        mols.append(mol)

    mylegends = ["CID " + str(x) for x in cids_top_unique]
    img = Draw.MolsToGridImage(mols, molsPerRow=2, subImgSize=(400,400), legends=mylegends)
    display(img)

    # Generate and save 3D structures of the top 3 compounds
    for idx, mycid in enumerate(cids_top_unique):
        if idx == 3 :
            break

        mysmiles = df[ df['CID'] == mycid ].IsomericSMILES.item()

        mymol = Chem.MolFromSmiles(mysmiles)
        mymol = Chem.AddHs(mymol)
        AllChem.EmbedMolecule(mymol)
        AllChem.MMFFOptimizeMolecule(mymol)

        filename = "screenig result" + str(idx) + "_" + str(mycid) + ".mol"
        Chem.MolToMolFile(mymol, filename)
    df.to_csv('screening result.csv')


    # Save the final filtered dataframe to CSV
    df_filtered.to_csv('screening_result.csv')

# Example usage:
#csv_file = "AID_1107225_datatable_active.csv"  # Path to your CSV file
#smiles_column = "PUBCHEM_EXT_DATASOURCE_SMILES"  # Index of the column containing SMILES strings
#virtual_screening_3(csv_file, smiles_column)
import pandas as pd
from rdkit import Chem
from rdkit.Chem import Descriptors


def process_csv_and_compute_RDKit_descriptors(input_csv, smiles_column):
    # Read CSV file
    df = pd.read_csv(input_csv)

    # Remove rows with empty cells in the SMILES column
    df = df.dropna(subset=[smiles_column])

    # Extract SMILES strings
    smiles_list = df[smiles_column].tolist()

    # Convert SMILES strings into RDKit molecules
    molecules = [Chem.MolFromSmiles(smi) for smi in smiles_list]

    # Calculate RDKit descriptors
    descriptor_names = [desc[0] for desc in Descriptors.descList]
    descriptor_values = [[desc[1](mol) for desc in Descriptors.descList] for mol in molecules]

    # Create DataFrame with descriptors
    descriptor_df = pd.DataFrame(descriptor_values, columns=descriptor_names)

    # Combine descriptors with original data
    df_with_descriptors = pd.concat([df.reset_index(drop=True), descriptor_df], axis=1)

    # Save the processed data to a new CSV file
    output_csv = input_csv.replace('.csv', '_processed.csv')
    df_with_descriptors.to_csv(output_csv, index=False)


# Example usage:
#input_csv = "AID_1860_datatable_inactive.csv"
#smiles_column = "SMILES"
#process_csv_and_compute_descriptors(input_csv, smiles_column)
import pandas as pd
import mordred
from rdkit import Chem
from mordred import Calculator, descriptors


def process_csv_and_compute_mordred_descriptors(input_csv, smiles_column):
    # Read CSV file
    df = pd.read_csv(input_csv)

    # Remove rows with empty cells in the SMILES column
    df = df.dropna(subset=[smiles_column])

    # Extract SMILES strings
    smiles_list = df[smiles_column].tolist()

    # Convert SMILES strings into RDKit molecules
    molecules = [Chem.MolFromSmiles(smi) for smi in smiles_list]

    # Create a Mordred calculator object
    calc = mordred.Calculator(mordred.descriptors, ignore_3D=False)

    # Compute descriptors for each molecule
    descriptor_values = calc.pandas(molecules)

    # Combine descriptors with original data
    df_with_descriptors = pd.concat([df.reset_index(drop=True), descriptor_values], axis=1)

    # Save the processed data to a new CSV file
    output_csv = input_csv.replace('.csv', '_processed.csv')
    df_with_descriptors.to_csv(output_csv, index=False)

    print(f"Processed data saved to {output_csv}")


# Example usage:
#input_csv = "AID_1860_datatable_inactive.csv"
#smiles_column = "SMILES"
#process_csv_and_compute_mordred_descriptors(input_csv, smiles_column)
#df = pd.read_csv("AID_1860_datatable_inactive_processed.csv")
#df


import pandas as pd
from rdkit import Chem
from rdkit import DataStructs
from rdkit.Chem import AllChem
import numpy as np


def process_csv_and_compute_morgan_fp(input_csv, smiles_column):
    # Read CSV file
    df = pd.read_csv(input_csv)

    # Remove rows with empty cells in the SMILES column
    df = df.dropna(subset=[smiles_column])

    # Extract SMILES strings
    smiles_list = df[smiles_column].tolist()

    # Convert SMILES strings into RDKit molecules and compute Morgan fingerprints
    Morgan_fpts = []
    for smi in smiles_list:
        mol = Chem.MolFromSmiles(smi)
        fpts = AllChem.GetMorganFingerprintAsBitVect(mol, 2, 2048)
        mfpts = np.array(fpts)
        Morgan_fpts.append(mfpts)

    # Create a DataFrame with Morgan fingerprints
    Morgan_fp_df = pd.DataFrame(Morgan_fpts)

    # Combine Morgan fingerprints with original data
    df_with_fp = pd.concat([df.reset_index(drop=True), Morgan_fp_df], axis=1)

    # Save the processed data to a new CSV file
    output_csv = input_csv.replace('.csv', '_with_morgan_fp.csv')
    df_with_fp.to_csv(output_csv, index=False)

    print(f"Processed data with Morgan fingerprints saved to {output_csv}")


# Example usage:
#input_csv = "AID_1860_datatable_inactive.csv"
#smiles_column = "SMILES"
#process_csv_and_compute_morgan_fp(input_csv, smiles_column)
import pandas as pd


def find_unique_values(csv_file, column_name):
    """
    Read a CSV file and find unique values in a specified column.

    Parameters:
        csv_file (str): Path to the CSV file.
        column_name (str): Name of the column to find unique values from.

    Returns:
        list: A list of unique values found in the specified column.
    """
    try:
        # Read the CSV file
        df = pd.read_csv(csv_file)

        # Find unique values in the specified column
        unique_values = df[column_name].unique().tolist()

        return unique_values
    except Exception as e:
        print(f"An error occurred: {e}")
        return []


# Example usage:
#csv_file = "pubchem_taxid_287_bioactivity.csv"  # Replace "example.csv" with the path to your CSV file
#column_name = "aid"  # Replace "AID" with the name of the column you want to find unique values from
#unique_values = find_unique_values(csv_file, column_name)
#print(unique_values)
import pandas as pd


def substance_sid_to_cid_1(*substance_sids):
    """Retrieve all AID URLs for given substance SIDs."""
    # Assuming substance_sid_conversion and get_text_from_url are defined elsewhere
    url = substance_sid_conversion(str(substance_sids[0]), record_type='cids/txt')
    cid = get_text_from_url(url[0])
    return cid


def compound_cid_to_CanonicalSMILES(compound_cids):
    print
    """Retrieve CanonicalSMILES for given compound CIDs."""
    # Assuming compound_cid_conversion is defined elsewhere
    url = compound_cid_conversion(*compound_cids, record_type="property/CanonicalSMILES/txt")
    smiles = get_text_from_url(url[0])

    return smiles


import pandas as pd


def substance_sid_to_cid_1(*substance_sids):
    """Retrieve all AID URLs for given substance SIDs."""
    # Assuming substance_sid_conversion and get_text_from_url are defined elsewhere
    url = substance_sid_conversion(str(substance_sids[0]), record_type='cids/txt')
    cid = get_text_from_url(url[0])
    return cid


def compound_cid_to_CanonicalSMILES(compound_cids):
    print
    """Retrieve CanonicalSMILES for given compound CIDs."""
    # Assuming compound_cid_conversion is defined elsewhere
    url = compound_cid_conversion(*compound_cids, record_type="property/CanonicalSMILES/txt")
    smiles = get_text_from_url(url[0])

    return smiles


def sid_to_smiles_csv(input_file, output_file):
    # Read the CSV file
    df = pd.read_csv(input_file)

    # Loop over 'sid' column and retrieve corresponding 'cid'
    cids = []
    for sid in df['SID']:
        cid = substance_sid_to_cid_1(sid)
        cids.append(cid)

    # Add 'cid' column to DataFrame
    df['cid'] = cids

    # Loop over 'cid' column and find corresponding 'CanonicalSMILES'
    smiles = []
    for cid in df['cid']:
        smile = compound_cid_to_CanonicalSMILES(cid)
        smiles.append(smile)

    # Add 'CanonicalSMILES' column to DataFrame
    df['CanonicalSMILES'] = smiles

    # Save DataFrame to CSV with new columns
    df.to_csv(output_file, index=False)

    # Display the DataFrame
    display(df)
    return


import pandas as pd
import pubchempy as pcp


def add_smiles_from_cid(csv_file, cid_column):
    def cid_to_smiles(cid):
        try:
            compound = pcp.Compound.from_cid(cid)
            smiles = compound.canonical_smiles
            return smiles
        except pcp.PubChemHTTPError:
            print(f"Could not retrieve SMILES for CID {cid}")
            return None

    # Load CSV file
    df = pd.read_csv(csv_file)

    # Loop over CID column and find corresponding SMILES
    smiles_list = []
    for cid in df[cid_column]:
        cid = str(int(cid))
        smiles = cid_to_smiles(cid)
        smiles_list.append(smiles)

    # Add the SMILES column to the DataFrame
    df['SMILES'] = smiles_list

    # Save the updated DataFrame to a new CSV file
    output_file = csv_file.split('.')[0] + '_with_SMILES.csv'
    df.to_csv(output_file, index=False)
    return df


# Example usage:
#add_smiles_from_cid('1.xlsx', 'cid')

import pandas as pd
import matplotlib.pyplot as plt
from rdkit import Chem
from rdkit.Chem import Draw
from rdkit.Chem import rdFMCS
from rdkit import DataStructs
from rdkit.Chem import AllChem
from sklearn.cluster import AgglomerativeClustering
import numpy as np

def highlight_substructure(mol, substructure_atoms):
    highlighted = Chem.Mol(mol)
    for atom in highlighted.GetAtoms():
        if atom.GetIdx() in substructure_atoms:
            atom.SetAtomMapNum(1)  # Mark atoms in substructure
        else:
            atom.SetAtomMapNum(0)
    return highlighted

def chemical_cluster_analysis(csv_file_path, smiles_column, similarity_threshold=0.8):
    # Read the CSV file
    df = pd.read_csv(csv_file_path)

    # Extract SMILES from the DataFrame
    smiles = df[smiles_column]

    # Convert SMILES to molecular objects
    mols = [Chem.MolFromSmiles(smi) for smi in smiles]

    # Generate Morgan fingerprints
    fps = [AllChem.GetMorganFingerprintAsBitVect(mol, 2, nBits=1024) for mol in mols]

    # Calculate similarity matrix
    similarity_matrix = np.zeros((len(mols), len(mols)))
    for i in range(len(mols)):
        for j in range(i, len(mols)):
            similarity_matrix[i][j] = similarity_matrix[j][i] = DataStructs.TanimotoSimilarity(fps[i], fps[j])

    # Perform hierarchical clustering with dynamic threshold
    clustering = AgglomerativeClustering(n_clusters=None, affinity='precomputed', linkage='average', distance_threshold=1-similarity_threshold)
    labels = clustering.fit_predict(similarity_matrix)

    # Get the number of clusters
    num_clusters = len(set(labels))

    # Visualize the chemicals in each cluster and find common substructure
    for label in range(num_clusters):  # Iterate over unique labels assigned by the clustering algorithm
        print(f"Cluster {label + 1}:")
        cluster_indices = [idx for idx, l in enumerate(labels) if l == label]
        mols_in_cluster = [mols[idx] for idx in cluster_indices]

        num_images = len(mols_in_cluster)
        num_cols = min(num_images, 5)  # Set maximum of 5 images per line
        num_rows = num_images // num_cols + (1 if num_images % num_cols != 0 else 0)

        fig, axes = plt.subplots(num_rows, num_cols, figsize=(num_cols * 5, num_rows * 5))
        axes = axes.flatten() if isinstance(axes, np.ndarray) else [axes]  # Ensure axes is a list

        for ax, mol, idx in zip(axes, mols_in_cluster, cluster_indices):
            if mol is not None:
                ax.imshow(Draw.MolToImage(mol, size=(200, 200)))
                ax.set_title(smiles[idx], fontsize=10)
                ax.axis('off')

                # Find common substructure for this cluster
                if len(mols_in_cluster) > 1:
                    mcs_result = rdFMCS.FindMCS(mols_in_cluster, threshold=0.5)
                    common_substructure = Chem.MolFromSmarts(mcs_result.smartsString)
                    if common_substructure is not None:
                        atom_indices = [atom.GetIdx() for atom in common_substructure.GetAtoms()]
                        highlighted_mol = highlight_substructure(mol, atom_indices)
                        ax.imshow(Draw.MolToImage(highlighted_mol, size=(200, 200)))

        # Find and display the common substructure for this cluster
        print("Common substructure for this cluster:")
        all_mols_in_cluster = [mol for mol in mols_in_cluster if mol is not None]
        if len(all_mols_in_cluster) > 1:
            mcs_result = rdFMCS.FindMCS(all_mols_in_cluster, threshold=0.7)
            common_substructure = Chem.MolFromSmarts(mcs_result.smartsString)
            if common_substructure is not None:
                print(Chem.MolToSmiles(common_substructure))
                display(common_substructure)
        else:
            print("There is only one chemical in this cluster, so there is no common substructure.")

        plt.tight_layout()
        plt.show()

# Example usage:
#chemical_cluster_analysis('smile pa.csv', smiles_column='SMILES', similarity_threshold=0.95)


import pandas as pd
import matplotlib.pyplot as plt
from rdkit import Chem
from rdkit.Chem import Draw
from rdkit.Chem import rdFMCS
from rdkit import DataStructs
from rdkit.Chem import AllChem
from sklearn.cluster import AgglomerativeClustering
import numpy as np

def highlight_substructure(mol, substructure_atoms):
    highlighted = Chem.Mol(mol)
    for atom in highlighted.GetAtoms():
        if atom.GetIdx() in substructure_atoms:
            atom.SetAtomMapNum(1)  # Mark atoms in substructure
        else:
            atom.SetAtomMapNum(0)
    return highlighted

def find_common_substructure(mols):
    # Find common substructure for the given list of molecules
    if len(mols) > 1:
        mcs_result = rdFMCS.FindMCS(mols, threshold=0.7)
        common_substructure = Chem.MolFromSmarts(mcs_result.smartsString)
        return common_substructure
    else:
        return None

def cluster_analysis_with_size(csv_file_path, smiles_column, cluster_size=10):
    # Read the CSV file
    df = pd.read_csv(csv_file_path)

    # Extract SMILES from the DataFrame
    smiles = df[smiles_column]

    # Convert SMILES to molecular objects
    mols = [Chem.MolFromSmiles(smi) for smi in smiles if Chem.MolFromSmiles(smi) is not None]

    # Generate Morgan fingerprints
    fps = [AllChem.GetMorganFingerprintAsBitVect(mol, 2, nBits=1024) for mol in mols]

    # Calculate pairwise Tanimoto similarities
    similarity_matrix = np.zeros((len(mols), len(mols)))
    for i in range(len(mols)):
        for j in range(i, len(mols)):
            similarity_matrix[i][j] = similarity_matrix[j][i] = DataStructs.TanimotoSimilarity(fps[i], fps[j])

    # Perform hierarchical clustering
    clustering = AgglomerativeClustering(n_clusters=None, affinity='precomputed', linkage='average', distance_threshold=1.0e9)
    labels = clustering.fit_predict(similarity_matrix)

    # Initialize clusters
    num_clusters = len(set(labels))
    clusters = [[] for _ in range(num_clusters)]

    # Assign molecules to clusters based on their similarity
    for idx, mol in enumerate(mols):
        label = labels[idx]
        clusters[label].append(mol)

    # Divide clusters into subclusters with specified size
    subclusters = []
    for cluster in clusters:
        if len(cluster) <= cluster_size:
            subclusters.append([cluster])
        else:
            num_subclusters = len(cluster) // cluster_size
            for i in range(num_subclusters):
                subcluster = cluster[i * cluster_size: (i + 1) * cluster_size]
                subclusters.append(subcluster)
            if len(cluster) % cluster_size != 0:
                subclusters.append(cluster[num_subclusters * cluster_size:])

    # Visualize clusters and find common substructures
    for idx, subcluster in enumerate(subclusters):
        print(f"Cluster {idx + 1}:")
        if len(subcluster) == 0:
            print("This cluster is empty.")
            continue
        # Visualize the chemicals in this cluster
        num_images = len(subcluster)
        num_cols = min(num_images, 5)  # Set maximum of 5 images per line
        num_rows = num_images // num_cols + (1 if num_images % num_cols != 0 else 0)

        fig, axes = plt.subplots(num_rows, num_cols, figsize=(num_cols * 5, num_rows * 5))
        axes = axes.flatten() if isinstance(axes, np.ndarray) else [axes]  # Ensure axes is a list

        for ax, mol in zip(axes, subcluster):
            if mol is not None:
                ax.imshow(Draw.MolToImage(mol, size=(200, 200)))
                ax.axis('off')

        plt.tight_layout()
        plt.show()

        # Find and display the common substructure for this cluster
        print("Common substructure for this cluster:")
        common_substructure = find_common_substructure(subcluster)
        if common_substructure is not None:
            print(Chem.MolToSmiles(common_substructure))
            display(common_substructure)
        else:
            print("There is no common substructure in this cluster.")

# Example usage:
#cluster_analysis_with_size('smile pa25.csv', smiles_column='SMILES', cluster_size=10)


import pandas as pd
import pubchempy as pcp


def cid_to_smiles(cid):
    try:
        compound = pcp.Compound.from_cid(cid)
        smiles = compound.canonical_smiles
        return smiles
    except pcp.PubChemHTTPError:
        print(f"Could not retrieve SMILES for CID {cid}")
        return None


def add_smiles_column(csv_file, cid_column):
    # Load CSV file
    df = pd.read_csv(csv_file)

    # Loop over CID column and find corresponding SMILES
    smiles_list = []
    for cid in df[cid_column]:
        if pd.notna(cid):  # Check if CID is not empty
            cid_str = str(int(cid))  # Convert CID to string
            smiles = cid_to_smiles(cid_str)  # Get SMILES for CID
        else:
            smiles = None  # If CID is empty, set SMILES to None
        smiles_list.append(smiles)

    # Add the SMILES column to the DataFrame
    df['SMILES'] = smiles_list

    # Save the updated DataFrame to a new CSV file
    output_file = csv_file.split('.')[0] + '_with_SMILES.csv'
    df.to_csv(output_file, index=False)
    display(df)

    print("SMILES column added and saved successfully!")


# Example usage:
#add_smiles_column('11.csv', 'cid')
