# -*- coding: utf-8 -*-
# file: omnigenome_model.py
# time: 18:36 06/04/2024
# author: YANG, HENG <hy345@exeter.ac.uk> (杨恒)
# github: https://github.com/yangheng95
# huggingface: https://huggingface.co/yangheng
# google scholar: https://scholar.google.com/citations?user=NPq5a_0AAAAJ&hl=en
# Copyright (C) 2019-2024. All Rights Reserved.
import json
import os
import warnings

import torch

from transformers import AutoModel, AutoConfig, AutoTokenizer

from ..misc.utils import fprint, env_meta_info


from ..misc.utils import RNA2StructureCache

rna2structure = RNA2StructureCache()


def count_parameters(model):
    return sum(p.numel() for p in model.parameters() if p.requires_grad)


def extract_last_hidden_state(model, inputs, ss=None, tokenizer=None):
    """

    :param model: The model to extract the last hidden state from
    :param inputs: The inputs to the model
    :param ss: We can use the secondary structure information to help the model to learn better representations,
                  if ss is not None, the model will NOT use the secondary structure information. If ss is 'viennarna',
                    the model will use the secondary structure information generated by the ViennaRNA package. If ss is
                    'model', the model will use the secondary structure information generated by the model itself.
    :param tokenizer: Required while ss='viennarna'. The tokenizer to tokenize the secondary structure information
    :return: The last hidden state of the model and the secondary structure information if ss is not None
    """
    assert ss in [
        None,
        "viennarna",
        "model",
    ], f'ss should be one of [None, "viennarna", "model"], got {ss}'
    if isinstance(inputs, tuple):
        input_ids = inputs[0]
        attention_mask = inputs[1]
    else:
        input_ids = inputs["input_ids"]
        attention_mask = inputs["attention_mask"]

    try:
        outputs = model(
            input_ids,
            attention_mask=attention_mask,
            output_hidden_states=True,
        )
    except Exception as e:
        # For autoregressive models, the attention_mask is not required
        outputs = model(
            input_ids,
            output_hidden_states=True,
        )

    assert (
        "last_hidden_state" in outputs
    ), f"last_hidden_state not found in the outputs from the {model.__class__.__name__}"
    last_hidden_state = outputs["last_hidden_state"]

    if ss == "viennarna":
        sequences = tokenizer.base_tokenizer.batch_decode(
            input_ids, skip_special_tokens=True
        )
        structures = [rna2structure.fold(seq.replace(" ", ""))[0] for seq in sequences]
        tokenized_struct = tokenizer(
            structures,
            padding="max_length",
            max_length=input_ids.shape[1],
            truncation=True,
            return_tensors="pt",
            add_special_tokens=False,
        )
        tokenized_struct.to(input_ids.device)
        ss_last_hidden_state = model(
            **tokenized_struct,
            output_hidden_states=True,
        )["last_hidden_state"]
        rna2structure.update_cache_file()
    elif ss == "model":
        raise NotImplementedError(
            "The model-based secondary structure information is not implemented yet."
        )
        # tokenized_struct = last_hidden_state.argmax(-1)
        # ss_last_hidden_state = model(
        #     tokenized_struct,
        #     attention_mask=attention_mask,
        #     output_hidden_states=True,
        # )
    else:
        return last_hidden_state

    return last_hidden_state, ss_last_hidden_state


class OmniGenomeModel(torch.nn.Module):
    def __init__(self, config, base_model, tokenizer, *args, **kwargs):
        label2id = kwargs.pop("label2id", None)
        if label2id is not None:
            config.label2id = label2id
            config.id2label = {v: k for k, v in config.label2id.items()}
            self.num_labels = len(config.label2id)

        trust_remote_code = kwargs.pop("trust_remote_code", True)

        super().__init__(*args, **kwargs)

        self.metadata = env_meta_info()
        self.metadata["model_cls"] = self.__class__.__name__
        self.config = config

        if isinstance(base_model, torch.nn.Module):
            self.model = base_model
        else:
            self.model = AutoModel.from_pretrained(
                base_model, trust_remote_code=trust_remote_code
            )

        self.model.config = config

        self.tokenizer = tokenizer
        if hasattr(self.tokenizer, "base_tokenizer"):
            self.pad_token_id = self.tokenizer.base_tokenizer.pad_token_id
        else:
            self.pad_token_id = self.tokenizer.pad_token_id

        self.num_labels = config.num_labels
        self.dropout = torch.nn.Dropout(kwargs.get("dropout", 0.0))
        self.activation = torch.nn.Tanh()

        for key, value in kwargs.items():
            self.metadata[key] = value

        fprint(
            f"The trainable parameters of the {self.model.__class__.__name__} model are: {count_parameters(self.model) / 1e6:.2f} Millions"
        )

    def loss_function(self, logits, labels):
        raise NotImplementedError(
            "The loss_function() function should be implemented for your model."
        )

    def predict(self, inputs, **kwargs):
        raise NotImplementedError(
            "The predict() function should be implemented for your model."
        )

    def inference(self, inputs, **kwargs):
        raise NotImplementedError(
            "The inference() function should be implemented for your model."
        )

    def forward(self, inputs):
        last_hidden_state = extract_last_hidden_state(self.model, inputs)
        last_hidden_state = self.dropout(last_hidden_state)
        last_hidden_state = self.activation(last_hidden_state)
        outputs = {"last_hidden_state": last_hidden_state}
        return outputs

    def __call__(self, inputs, labels=None, *args, **kwargs):
        if isinstance(inputs, dict):
            labels = inputs.get("labels", None)
            label = inputs.get("label", None)
            labels = labels if labels is not None else label
            if labels is None:
                warnings.warn(
                    "No labels are provided in the inputs, the model will not calculate the loss."
                )
        elif isinstance(inputs, tuple):
            labels = inputs[1]
            inputs = inputs[0]
        elif labels is not None:
            labels = labels

        outputs = self.forward(inputs)

        if labels is not None:
            outputs["loss"] = self._calculate_loss(outputs, labels)
        else:
            outputs["loss"] = None
        return outputs

    def _calculate_loss(self, outputs, labels):
        loss = outputs.get("loss", None)
        if loss is not None:
            return outputs

        logits = outputs["logits"]
        if logits is not None or labels is not None:
            loss = self.loss_function(logits, labels)
            return loss
        else:
            raise RuntimeError(
                "The output of the forward() function should be a dictionary-like objective"
                " and have either 'loss', or 'logits' and 'labels' attribute."
            )

    def save(self, path, overwrite=False, **kwargs):
        self.eval()
        import dill

        if os.path.exists(path) and not overwrite:
            raise FileExistsError(
                f"The path {path} already exists, please set overwrite=True to overwrite it."
            )

        if not os.path.exists(path):
            os.makedirs(path)

        device = self.model.device

        self.model.to("cpu")
        with open(f"{path}/tokenizer.pkl", "wb") as f:
            dill.dump(self.tokenizer, f)
        with open(f"{path}/metadata.json", "w", encoding="utf8") as f:
            json.dump(self.metadata, f)
        self.model.save_pretrained(
            f"{path}", safe_serialization=False
        )  # do not remove this line, used to save customed model scripts
        with open(f"{path}/pytorch_model.bin", "wb") as f:
            torch.save(self.state_dict(), f)
        self.model.to(device)
        fprint(f"The model is saved to {path}.")

    def load(self, path, **kwargs):
        with open(f"{path}/metadata.json", "r", encoding="utf8") as f:
            metadata = json.load(f)

        if metadata["model_cls"] != self.__class__.__name__:  # Check the model class
            raise ValueError(
                f"The model class in the loaded model is {metadata['model_cls']}, "
                f"but the current model class is {self.__class__.__name__}."
            )
        config = AutoConfig.from_pretrained(path, trust_remote_code=True, **kwargs)

        for key, value in config.__dict__.items():
            if key not in self.config.__dict__ or self.config.__dict__[key] != value:
                fprint(
                    f"Warning: The value of the key {key} in the loaded model is {value}, "
                    f"but the current value is {self.config.__dict__.get(key, None)}."
                )

        with open(f"{path}/pytorch_model.bin", "rb") as f:
            self.load_state_dict(
                torch.load(f, map_location=kwargs.get("device", "cpu")), strict=True
            )
        return self

    def load_tokenizer(self, path):
        import dill

        with open(f"{path}/tokenizer.pkl", "rb") as f:
            tokenizer = dill.load(f)
        return tokenizer

    @staticmethod
    def from_pretrained(model_name_or_path, tokenizer, *args, **kwargs):
        config = kwargs.pop("config", None)
        if config is None:
            config = AutoConfig.from_pretrained(model_name_or_path, **kwargs)
        base_model = AutoModel.from_pretrained(model_name_or_path, **kwargs)
        if tokenizer is None:
            tokenizer = AutoTokenizer.from_pretrained(base_model, **kwargs)
        return OmniGenomeModel(config, base_model, tokenizer, *args, **kwargs)
