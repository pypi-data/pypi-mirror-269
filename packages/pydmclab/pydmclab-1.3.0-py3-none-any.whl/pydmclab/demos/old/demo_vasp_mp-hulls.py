import os
import numpy as np

from pydmc.utils.handy import read_json, write_json, make_sub_for_launcher
from pydmc.core.query import MPQuery
from pydmc.core.mag import MagTools
from pydmc.core.struc import StrucTools
from pydmc.hpc.launch import LaunchTools
from pydmc.hpc.submit import SubmitTools
from pydmc.hpc.analyze import AnalyzeBatch

"""
This demo:
    1) Let's get all the compounds in the Li-Mn-Cl space meeting some criteria 
    2) Then calculate them with metagga and MP standards
"""
"""
Overall note:
    - some of the configurations I'm choosing are purely to illustrate how to change things
        - not necessarily the best choices for a given calculation

"""

############### GLOBAL VARIABLES ################
"""
We'll define a few global variables here ####
 - note: generally we don't want to do this, 
    - but it's convenient for a few things (
    - e.g., specifying where to save data generated by many functions)
"""

# where is this file
SCRIPTS_DIR = os.getcwd()

# where are my calculations going to live
CALCS_DIR = SCRIPTS_DIR.replace('scripts', 'calcs')

# where is my data going to live
DATA_DIR = SCRIPTS_DIR.replace('scripts', 'data')

for d in [CALCS_DIR, DATA_DIR]:
    if not os.path.exists(d):
        os.makedirs(d)

# if you need data from MP as a starting point (often the case), you need your API key
API_KEY = 'N3KdATtMmcsUL94g'

# lets put a tag on all the files we save
FILE_TAG = 'mp-hulls'

############### QUERYING MP ################

"""
One of the main things you must provide for this workflow is a starting crystal structure
    - oftentimes, we use Materials Project to grab initial structures
    - we then either calculate them directly using different approaches
    - or we manipulate them to make new structures
    
If we want MP data, we use pydmc.core.query.MPQuery
""" 

"""
One of the main things you must provide for this workflow is a starting crystal structure
    - oftentimes, we use Materials Project to grab initial structures
    - we then either calculate them directly using different approaches
    - or we manipulate them to make new structures
    
If we want MP data, we use pydmc.core.query.MPQuery
""" 

"""
Query example:
    - we want all entries in the Li-Mn-Cl-F chemical space
        - restricted to entries within 20 meV/atom of the convex hull
        - restricted to <= 3 lowest energy polymorphs per composition
        - restricted to structures with < 50 atoms in their unit cells
        - excluding elemental phases (Li1, Mn1, O1)
    - this might be a query if you were doing the convex hull analysis
"""
def get_query(comp='Li-Mn-Cl-F',
              only_gs=False,
              max_Ehull=0.02,
              max_strucs_per_cmpd=3,
              max_sites_per_structure=50,
              criteria={'nelements' : {'$gte' : 2}},
              include_structure=True,
              supercell_structure=False,
              savename='query_%s.json' % FILE_TAG,
              remake=False):
    
    fjson = os.path.join(DATA_DIR, savename)
    if os.path.exists(fjson) and not remake:
       return read_json(fjson)
    
    mpq = MPQuery(api_key=API_KEY)
    
    data = mpq.get_data_for_comp(comp=comp,
                                 only_gs=only_gs,
                                 max_Ehull=max_Ehull,
                                 max_strucs_per_cmpd=max_strucs_per_cmpd,
                                 max_sites_per_structure=max_sites_per_structure,
                                 criteria=criteria,
                                 include_structure=include_structure,
                                 supercell_structure=supercell_structure)
    
    write_json(data, fjson)   
    return read_json(fjson)

def check_query(query):
    for mpid in query:
        print('\nmpid: %s' % mpid)
        print('\tcmpd: %s' % query[mpid]['cmpd'])
        print('\tEhull: %.2f eV/atom' % query[mpid]['Ehull_mp'])
        
############### MAGMOMS ################
"""
MP sometimes does AFM, but generally they use FM... we'll just use FM for this example
    
"""        
            
############### LAUNCHING ################
"""
We have our structures and we have their MAGMOMs (because it's AFM)

Now, we can let pydmc set up all our calculations (once we specify some configurations)

In the first step, we'll figure out which "launch directories" we need to make and which VASP calculations will belong to each launch directory
    - in general, a VASP calculation is defined by a distinct:
        - POSCAR: the starting crystal structure
        - INCAR: the input parameters for VASP
        - KPOINTS: the k-point mesh
        - POTCAR: the pseudopotentials
    - oftentimes, we'll chain calculations together
        - for instance, we might run a geometry optimization ("relax") then a static calculation at that geometry ("static")
            - these "chains" will get "launched" sequentially
            - this will happen automatically so that the static starts only once the relax is finished
            - we will also pass information (input/output files) between these chained calculations that belong to a single launch directory
    - each launch directory will have one chain of calculations

This function will return something that looks like:
    {launch directory name (str, top_level/ID/standard/final_xc/mag/) 
        : [list of VASP calculations to run (e.g., gga-relax)]}
        
Note on launch_directory naming:
    - top_level = chemical formula or similar that describes a group of structures
    - ID = unique ID that describes the structure
    - standard = pydmc standards to generate input files
    - final_xc = the final exchange-correlation method we want to get energies for
    - mag = nm, fm, or afm_*
    
Note on user_configs:
    - we'll need to specify these if we want anything beside the default configuration
        - defaults found at pydmc/data/data/_launch_configs.yaml
    - set these in main()
"""

def get_launch_dirs(query,
                    user_configs,
                    magmoms=None,
                    make_launch_dirs=True,
                    savename='launch_dirs_%s.json' % FILE_TAG,
                    remake=False):
    
    fjson = os.path.join(DATA_DIR, savename)
    if os.path.exists(fjson) and not remake:
        return read_json(fjson)
    
    all_launch_dirs = {}
    for mpid in query:

        structure = query[mpid]['structure']
        top_level = query[mpid]['cmpd']
        ID = mpid
        
        launch = LaunchTools(calcs_dir=CALCS_DIR,
                             structure=structure,
                             magmoms=magmoms,
                             top_level=top_level,
                             unique_ID=ID,
                             user_configs=user_configs)

        launch_dirs = launch.launch_dirs_to_tags

        if make_launch_dirs:
            launch.create_launch_dirs_and_make_POSCARs

        all_launch_dirs = {**all_launch_dirs, **launch_dirs}

    write_json(all_launch_dirs, fjson)   
    return read_json(fjson)   

def check_launch_dirs(launch_dirs):
    print('\nanalyzing launch directories')
    for d in launch_dirs:
        print('\nlaunching from %s' % d)
        print('   these calcs: %s' % launch_dirs[d])

############### SUBMISSION ################
"""
Now that we have our launch directories,
    - we can prepare each VASP calculation 
    - write a submission script to launch each chain of calculations
    - submit the calculations to the queue

This function won't return anything, but it will submit jobs to the queue

Note on user_configs:
    - we'll need to specify these if we want anything beside the default configurations
        - defaults for submission parameters found at pydmc/data/data/_sub_configs.yaml
        - defaults for slurm parameters found at pydmc/data/data/_slurm_configs.yaml
        - defaults for VASP parameters found at pydmc/data/data/_vasp_configs.yaml
    - set these in main()
"""

def submit_calcs(launch_dirs,
                 user_configs,
                 magmoms=None,
                 ready_to_launch=True):
    for launch_dir in launch_dirs:

        # these are calcs that should be chained in this launch directory
        valid_calcs = launch_dirs[launch_dir]

        # these are some configurations we'll extract from the launch directory name
        top_level, ID, standard, final_xc, mag = launch_dir.split('/')[-5:]
        user_configs['standard'] = standard
        user_configs['xc'] = final_xc
        user_configs['mag'] = mag
        user_configs['job-name'] = '.'.join([top_level, ID, standard, final_xc, mag])

        # we need to pass the right MAGMOM if the calc is AFM
        if 'afm' in mag:
            idx = mag.split('_')[1]
            if ID in magmoms:
                magmom = magmoms[ID][idx]
            elif top_level in magmoms:
                magmom = magmoms[top_level][ID][idx]
        else:
            magmom = None

        user_configs['magmom'] = magmom
        
        # now we'll prep the VASP directories and write the submission script
        sub = SubmitTools(launch_dir=launch_dir,
                          valid_calcs=valid_calcs,
                          user_configs=user_configs)

        sub.write_sub
        
        # if we're "ready to launch", let's launch
        if ready_to_launch:
            sub.launch_sub    
            
def check_subs(launch_dirs,
                fsub='sub.sh'):
    print('\nanalyzing submission scripts')
    launch_dirs_to_check = list(launch_dirs.keys())
    if len(launch_dirs_to_check) > 6:
        launch_dirs_to_check = launch_dirs_to_check[:3] + launch_dirs_to_check[-3:]

    for d in launch_dirs_to_check:
        submission_file = os.path.join(d, fsub)
        with open(submission_file) as f:
            print('\nanalyzing %s' % submission_file)
            for line in f:
                if 'working' in line:
                    print(line)
                    
                    
############### ANALYSIS ################
    
"""
The last step is to analyze the calculations as they run

Depending on what you're goal is, there are a lot of additional analyses that might be useful.

The goal of VASPOutputs, AnalyzeVASP, and AnalyzeBatch are to save a lot of the HPC data into
    more tractable json's that can be moved locally for more analysis
    
This function will return a dictionary that looks like:
    {top_level.ID.standard.final_xc.mag.xc_calc: 
        {OUTPUT DATA} for calculations in launch_dir}
Note on user_configs:
    - we'll need to specify these if we want anything beside the default configurations
        - defaults for what to analyze can be found at pydmc/data/data/_batch_vasp_analysis_configs.yaml
    - do we want magnetization, DOS, etc.
    - do we only want static calculations, etc
    - set these in main()
"""

def analyze_calcs(launch_dirs,
                  user_configs,
                  savename='results_%s.json' % FILE_TAG,
                  remake=False):
    
    fjson = os.path.join(DATA_DIR, savename)
    if os.path.exists(fjson) and not remake:
        return read_json(fjson)
    
    analyzer = AnalyzeBatch(launch_dirs,
                            user_configs=user_configs)

    data = analyzer.results

    write_json(data, fjson)   
    return read_json(fjson)    

def check_results(results):

    keys_to_check = list(results.keys())

    converged = 0
    for key in keys_to_check:
        top_level, ID, standard, xc, mag, xc_calc = key.split('.')
        data = results[key]
        convergence = results[key]['results']['convergence']
        print('\n%s' % key)
        print('convergence = %s' % convergence)
        if convergence:
            converged += 1
            print('E (static) = %.2f' % data['results']['E_per_at'])
            print('E (relax) = %.2f' % data['meta']['E_relax'])
            #print(data['structure'])
    
    print('\n\n %i/%i converged' % (converged, len(keys_to_check)))       
            
def main():
    """
    It's generally a good idea to set True/False statements at the top
        - this will allow you to quickly toggle whether or not to repeat certain steps
    """    
    remake_sub_launch = False
    
    remake_query = False
    print_query_check = True 
    
    remake_launch_dirs = False
    print_launch_dirs_check = True
    
    remake_subs = True
    ready_to_launch = True
    print_subs_check = True
    
    remake_results = True
    print_results_check = True
    
    """
    Sometimes we'll need to run our launch script on a compute node if generating magmoms or analyzing directories takes a while
        here, we'll create a file called sub_launch.sh
        you can then execute this .py file on a compute node with:
            $ sbatch sub_launchs.h
    """   
    if remake_sub_launch or not os.path.exists(os.path.join(os.getcwd(), 'sub_launch.sh')):
        make_sub_for_launcher()

    query = get_query(remake=remake_query)

    if print_query_check:
        check_query(query=query)
    
    """
    Here, I'll specify the user_configs pertaining to setting up the launch directories
        - no AFM

        - setting standards=['dmc'] will also do DMC-standard calcs
            - setting final_xcs=['metagga'] means we'll do meta-gga w/ our DMC standards
        - and we want to compare_to_mp
            - this means we will *additionally* do MP-consistent GGA+U         
    """
    launch_configs = {'n_afm_configs' : 0,
                      'standards' : ['dmc'],
                      'final_xcs' : ['metagga'],
                      'compare_to_mp' : True}
    
    launch_dirs = get_launch_dirs(query=query,
                                  user_configs=launch_configs,
                                  remake=remake_launch_dirs)
    if print_launch_dirs_check:
        check_launch_dirs(launch_dirs=launch_dirs)
        
    """
    Now, we need to specify any configurations relevant to VASP set up or our submission scripts
    For this example, we'll do the following (on top of the defaults):
        - run on only 8 cores
        - run with a walltime of 48 hours
        - make sure we don't run LOBSTER
        - turn off symmetry in our loose, relax, and static calculations
        
    """
    user_configs = {'ntasks' : 8,
                    'time' : int(48*60),
                    'lobster_static' : False}
    
    if remake_subs:
        submit_calcs(launch_dirs=launch_dirs,
                     user_configs=user_configs,
                     ready_to_launch=ready_to_launch)
 
    if print_subs_check:
        check_subs(launch_dirs=launch_dirs)
    
    analysis_configs = {'n_procs' : 4}
    results = analyze_calcs(launch_dirs=launch_dirs,
                            user_configs=analysis_configs,
                            remake=remake_results)
    if print_results_check:
        check_results(results)

if __name__ == '__main__':
    main()

