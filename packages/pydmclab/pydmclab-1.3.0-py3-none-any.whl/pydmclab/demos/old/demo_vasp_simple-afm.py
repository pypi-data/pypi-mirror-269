import os
import numpy as np

from pydmc.utils.handy import read_json, write_json, make_sub_for_launcher
from pydmc.core.query import MPQuery
from pydmc.core.mag import MagTools
from pydmc.core.struc import StrucTools
from pydmc.hpc.launch import LaunchTools
from pydmc.hpc.submit import SubmitTools
from pydmc.hpc.analyze import AnalyzeBatch

"""
This demo:
    - Goal: compute a few afm configurations for IrO2 and RuO2
    
    - Approach:
        1) Get the ground-state structures for IrO2 and RuO2 from MP
        2) Make it a supercell to allow for sampling AFM configurations
        3) Enumerate some plauslabe AFM orderings for each Ru/Ir ordering for each x
        4) Calculate these with GGA+U and r2SCAN
        5) Collect the results        
        
Note:
    - some of the configurations I'm choosing are purely to illustrate how to change things
        - not necessarily the best choices for a given calculation
"""

############### GLOBAL VARIABLES ################
"""
We'll define a few global variables here ####
 - note: generally we don't want to do this, 
    - but it's convenient for a few things 
    - e.g., specifying where to save data generated by many functions
"""

# where is this file
SCRIPTS_DIR = os.getcwd()

# where are my calculations going to live
CALCS_DIR = SCRIPTS_DIR.replace('scripts', 'calcs')

# where is my data going to live
DATA_DIR = SCRIPTS_DIR.replace('scripts', 'data')

for d in [CALCS_DIR, DATA_DIR]:
    if not os.path.exists(d):
        os.makedirs(d)

# if you need data from MP as a starting point (often the case), you need your API key
API_KEY = 'N3KdATtMmcsUL94g'

# lets put a tag on all the files we save
FILE_TAG = 'simple-afm'

############### QUERYING MP ################

"""
One of the main things you must provide for this workflow is a starting crystal structure
    - oftentimes, we use Materials Project to grab initial structures
    - we then either calculate them directly using different approaches
    - or we manipulate them to make new structures
    
If we want MP data, we use pydmc.core.query.MPQuery
""" 

"""
Let's say we want a 2x1x1 supercell of the ground-state polymorphs of RuO2 and IrO2

This function will return something that looks like this:
    {mpid (str, mp-****) : 
        {'cmpd' : chemical formula (str),
        'structure : Structure.as_dict(),
        'E_mp' : energy per atom from MP,
        'Ef_mp' : formation energy per atom from MP,
        etc}
"""
def get_query(comp=['MoO2', 'TiO2'],
              only_gs=True,
              include_structure=True,
              supercell_structure=[2,1,1],
              savename='query_%s.json' % FILE_TAG,
              remake=False):
    
    fjson = os.path.join(DATA_DIR, savename)
    if os.path.exists(fjson) and not remake:
       return read_json(fjson)
    
    mpq = MPQuery(api_key=API_KEY)
    
    data = mpq.get_data_for_comp(comp=comp,
                                 only_gs=only_gs,
                                 include_structure=include_structure,
                                 supercell_structure=supercell_structure)
    
    write_json(data, fjson) 
    return read_json(fjson)

"""
It's generally a good idea to write a quick "check" function to make sure each function does what you want
"""
def check_query(query):
    for mpid in query:
        print('\nmpid: %s' % mpid)
        print('\tcmpd: %s' % query[mpid]['cmpd'])
        print('\tstructure has %i sites' % len(StrucTools(query[mpid]['structure']).structure))
    
############### MAGMOMS ################
"""
Oftentimes we're interested in magnetic (spin-polarized) calculations

In the pydmc language, we have three types of initial magnetic configurations:
    - "nm" = nonmagnetic (not spin-polarized)
    - "fm" = ferromagnetic (all spins aligned in the positive direction)
    - "afm_*" = antiferromagnetic (half spins aligned in the positive direction, half in the negative direction)
        - you can imagine that there are many unique ways to make an afm configuration
        - the "*" is a placeholder for an integer (configuration 0, 1, 2, 3 = afm_0, afm_1, afm_2, afm_3)

If we're only interested in nm or fm calculations, you don't have to do anything as pydmc takes care of it all

If we want afm configurations, these are nontrivial to enumerate, so we want to enumerate them once and save to a file.

We'll generate a "magmoms" dictionary for some afm configurations for the compounds in query_example_1

Sometimes when we enumerate possible magnetic configurations, there are an absurdly large number
    - usually it's ok if we just sample some representative set of ~1-10 configurations
    
If we want to run VASP on N configurations, it's probably a good idea to set 
    - max_afm_combos at least 3*N in case some we generate are symmetrically equivalent
    
This function will return something that looks like:
    magmoms = {mpid (str, mp-***) : 
                {AFM configuration index (int) :
                    [magmoms (list of floats, len = # of sites)]}} 
    
"""

def get_magmoms(query,
                max_afm_combos=20,
                savename='magmoms_%s.json' % FILE_TAG,
                remake=False):
                          
    fjson = os.path.join(DATA_DIR, savename)
    if not remake and os.path.exists(fjson):
        return read_json(fjson)
    
    magmoms = {}
    for mpid in query:
        magmoms[mpid] = {}
        structure = query[mpid]['structure']
        magtools = MagTools(structure=structure,
                            max_afm_combos=max_afm_combos)
        curr_magmoms = magtools.get_afm_magmoms
        magmoms[mpid] = curr_magmoms

    write_json(magmoms, fjson) 
    return read_json(fjson)

def check_magmoms(query,
                  magmoms):
    for mpid in magmoms:
        cmpd = query[mpid]['cmpd']
        curr_magmoms = magmoms[mpid]
        print('\nanalyzing magmoms')
        print('%s: %i AFM configs\n' % (cmpd, len(curr_magmoms)))            
            
############### LAUNCHING ################
"""
We have our structures and we have their MAGMOMs (because it's AFM)

Now, we can let pydmc set up all our calculations (once we specify some configurations)

In the first step, we'll figure out which "launch directories" we need to make and which VASP calculations will belong to each launch directory
    - in general, a VASP calculation is defined by a distinct:
        - POSCAR: the starting crystal structure
        - INCAR: the input parameters for VASP
        - KPOINTS: the k-point mesh
        - POTCAR: the pseudopotentials
    - oftentimes, we'll chain calculations together
        - for instance, we might run a geometry optimization ("relax") then a static calculation at that geometry ("static")
            - these "chains" will get "launched" sequentially
            - this will happen automatically so that the static starts only once the relax is finished
            - we will also pass information (input/output files) between these chained calculations that belong to a single launch directory
    - each launch directory will have one chain of calculations

This function will return something that looks like:
    {launch directory name (str, top_level/ID/standard/final_xc/mag/) 
        : [list of VASP calculations to run (e.g., gga-relax)]}
        
Note on launch_directory naming:
    - top_level = chemical formula or similar that describes a group of structures (in this case, just each formula)
    - ID = unique ID that describes the structure (in this case, mpid)
    - standard = pydmc standards to generate input files
    - final_xc = the final exchange-correlation method we want to get energies for
    - mag = nm, fm, or afm_*
    
Note on user_configs:
    - we'll need to specify these if we want anything beside the default configuration
        - defaults found at pydmc/data/data/_launch_configs.yaml
    - set these in main()

Note on refresh_configs:
    - this will remove the _*configs.yaml files from os.getcwd() and write new ones
    - generally this is OK, just don't expect manual edits to the yamls to do anything
"""

def get_launch_dirs(query,
                    user_configs,
                    magmoms,
                    make_launch_dirs=True,
                    refresh_configs=True,
                    savename='launch_dirs_%s.json' % FILE_TAG,
                    remake=False):
    
    fjson = os.path.join(DATA_DIR, savename)
    if os.path.exists(fjson) and not remake:
        return read_json(fjson)
    
    all_launch_dirs = {}
    for mpid in query:

        structure = query[mpid]['structure']
        curr_magmoms = magmoms[mpid]
        top_level = query[mpid]['cmpd']
        ID = mpid
        
        launch = LaunchTools(calcs_dir=CALCS_DIR,
                             structure=structure,
                             magmoms=curr_magmoms,
                             top_level=top_level,
                             unique_ID=ID,
                             user_configs=user_configs,
                             refresh_configs=refresh_configs)

        launch_dirs = launch.launch_dirs_to_tags

        if make_launch_dirs:
            launch.create_launch_dirs_and_make_POSCARs

        all_launch_dirs = {**all_launch_dirs, **launch_dirs}

    write_json(all_launch_dirs, fjson) 
    return read_json(fjson)  

def check_launch_dirs(launch_dirs):
    print('\nanalyzing launch directories')
    for d in launch_dirs:
        print('\nlaunching from %s' % d)
        print('   these calcs: %s' % launch_dirs[d])

############### SUBMISSION ################
"""
Now that we have our launch directories,
    - we can prepare each VASP calculation 
    - write a submission script to launch each chain of calculations
    - submit the calculations to the queue

This function won't return anything, but it will submit jobs to the queue

Note on user_configs:
    - we'll need to specify these if we want anything beside the default configurations
        - defaults for submission parameters found at pydmc/data/data/_sub_configs.yaml
        - defaults for slurm parameters found at pydmc/data/data/_slurm_configs.yaml
        - defaults for VASP parameters found at pydmc/data/data/_vasp_configs.yaml
    - set these in main()
"""

def submit_calcs(launch_dirs,
                 user_configs,
                 magmoms,
                 refresh_configs=['vasp', 'sub', 'slurm'],
                 ready_to_launch=True):
    for launch_dir in launch_dirs:

        # these are calcs that should be chained in this launch directory
        valid_calcs = launch_dirs[launch_dir]

        # these are some configurations we'll extract from the launch directory name
        top_level, ID, standard, final_xc, mag = launch_dir.split('/')[-5:]
        user_configs['standard'] = standard
        user_configs['xc_to_run'] = final_xc
        user_configs['mag'] = mag
        user_configs['job-name'] = '.'.join([top_level, ID, standard, final_xc, mag])

        # we need to pass the right MAGMOM if the calc is AFM
        if 'afm' in mag:
            idx = mag.split('_')[1]
            if ID in magmoms:
                magmom = magmoms[ID][idx]
            elif top_level in magmoms:
                magmom = magmoms[top_level][ID][idx]
        else:
            magmom = None

        user_configs['magmom'] = magmom
        
        # now we'll prep the VASP directories and write the submission script
        sub = SubmitTools(launch_dir=launch_dir,
                          valid_calcs=valid_calcs,
                          user_configs=user_configs,
                          refresh_configs=refresh_configs)

        sub.write_sub
        
        # if we're "ready to launch", let's launch
        if ready_to_launch:
            sub.launch_sub    
            
def check_subs(launch_dirs,
                fsub='sub.sh'):
    print('\nanalyzing submission scripts')
    launch_dirs_to_check = list(launch_dirs.keys())
    if len(launch_dirs_to_check) > 6:
        launch_dirs_to_check = launch_dirs_to_check[:3] + launch_dirs_to_check[-3:]

    for d in launch_dirs_to_check:
        submission_file = os.path.join(d, fsub)
        with open(submission_file) as f:
            print('\nanalyzing %s' % submission_file)
            for line in f:
                if 'working' in line:
                    print(line)
                    
                    
############### ANALYSIS ################
    
"""
The last step is to analyze the calculations as they run

Depending on what you're goal is, there are a lot of additional analyses that might be useful.

The goal of VASPOutputs, AnalyzeVASP, and AnalyzeBatch are to save a lot of the HPC data into
    more tractable json's that can be moved locally for more analysis
    
This function will return a dictionary that looks like:
    {top_level.ID.standard.final_xc.mag.xc_calc: 
        {OUTPUT DATA} for calculations in launch_dir}

Note on user_configs:
    - we'll need to specify these if we want anything beside the default configurations
        - defaults for what to analyze can be found at pydmc/data/data/_batch_vasp_analysis_configs.yaml
    - do we want magnetization, DOS, etc.
    - do we only want static calculations, etc
    - set these in main()
"""

def analyze_calcs(launch_dirs,
                  user_configs,
                  refresh_configs=True,
                  savename='results_%s.json' % FILE_TAG,
                  remake=False):
    
    fjson = os.path.join(DATA_DIR, savename)
    if os.path.exists(fjson) and not remake:
        return read_json(fjson)
    
    analyzer = AnalyzeBatch(launch_dirs,
                            user_configs=user_configs,
                            refresh_configs=refresh_configs)

    data = analyzer.results

    write_json(data, fjson) 
    return read_json(fjson)

def check_results(results):

    keys_to_check = list(results.keys())

    converged = 0
    for key in keys_to_check:
        top_level, ID, standard, xc, mag, xc_calc = key.split('.')
        data = results[key]
        convergence = results[key]['results']['convergence']
        print('\n%s' % key)
        print('convergence = %s' % convergence)
        if convergence:
            converged += 1
            print('E (static) = %.2f' % data['results']['E_per_at'])
            print('E (relax) = %.2f' % data['meta']['E_relax'])
            print('EDIFFG = %i' % data['meta']['incar']['EDIFFG'])
            print('1st POTCAR = %s' % data['meta']['potcar'][0])
            if mag != 'nm':
                magnetization = data['magnetization']
                an_el = list(magnetization.keys())[0]
                an_idx = list(magnetization[an_el].keys())[0]
                that_mag = magnetization[an_el][an_idx]['mag']
                print('mag on %s (%s) = %.2f' % (an_el, str(an_idx), that_mag))
            print(data['structure'])
    
    print('\n\n %i/%i converged' % (converged, len(keys_to_check)))       
            
def main():
    """
    It's generally a good idea to set True/False statements at the top
        - this will allow you to quickly toggle whether or not to repeat certain steps
    """    
    remake_sub_launch = False
    
    remake_query = False
    print_query_check = True 
    
    remake_magmoms = False
    print_magmoms_check = True
    
    remake_launch_dirs = False
    print_launch_dirs_check = True
    
    remake_subs = True
    ready_to_launch = True
    print_subs_check = True
    
    remake_results = True
    print_results_check = True
    
    """
    Sometimes we'll need to run our launch script on a compute node if generating magmoms or analyzing directories takes a while
        here, we'll create a file called sub_launch.sh
        you can then execute this .py file on a compute node with:
            $ sbatch sub_launchs.h
    """   
    if remake_sub_launch or not os.path.exists(os.path.join(os.getcwd(), 'sub_launch.sh')):
        make_sub_for_launcher()

    query = get_query(remake=remake_query)

    if print_query_check:
        check_query(query=query)
        
    
    magmoms = get_magmoms(query=query,
                          remake=remake_magmoms)

    if print_magmoms_check:
        check_magmoms(query=query,
                      magmoms=magmoms)
    
    """
    Here, I'll specify the user_configs pertaining to setting up the launch directories
        - let's consider 1 AFM configuration
        - let's use DMC standards
        - and let's compare GGA+U to METAGGA
    """
    launch_configs = {'n_afm_configs' : 1,
                      'standards' : ['dmc'],
                      'final_xcs' : ['ggau', 'metagga']}
    
    launch_dirs = get_launch_dirs(query=query,
                                  user_configs=launch_configs,
                                  magmoms=magmoms,                                  
                                  remake=remake_launch_dirs)
    if print_launch_dirs_check:
        check_launch_dirs(launch_dirs=launch_dirs)
        
    """
    Now, we need to specify any configurations relevant to VASP set up or our submission scripts
    For this example, we'll do the following (on top of the defaults):
        - run on only 8 cores
        - run with a walltime of 80 hours
        - make sure we run LOBSTER
        - use a slightly higher ENCUT in all our calculations
        
    """
    user_configs = {'ntasks' : 8,
                    'time' : int(80*60),
                    'lobster_static' : True,
                    'relax_incar' : {'ENCUT' : 555},
                    'static_incar' : {'ENCUT' : 555},
                    'loose_incar' : {'ENCUT' : 555}}
    
    if remake_subs:
        submit_calcs(launch_dirs=launch_dirs,
                     user_configs=user_configs,
                     magmoms=magmoms,                    
                     ready_to_launch=ready_to_launch)
 
    if print_subs_check:
        check_subs(launch_dirs=launch_dirs)
        
    """
    Now, we can specify what we want to collect from our calculations
        - let's run in parallel w/ 4 processors
        - include metadata
        - include magnetization results
        
    """
    
    analysis_configs = {'n_procs' : 4,
                        'include_meta' : True,
                        'include_mag' : True}
    results = analyze_calcs(launch_dirs=launch_dirs,
                            user_configs=analysis_configs,
                            remake=remake_results)
    if print_results_check:
        check_results(results)

if __name__ == '__main__':
    main()

