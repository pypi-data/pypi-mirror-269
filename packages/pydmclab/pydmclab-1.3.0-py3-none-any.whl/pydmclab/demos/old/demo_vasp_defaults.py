import os
import numpy as np

from pydmc.utils.handy import read_json, write_json, make_sub_for_launcher
from pydmc.core.query import MPQuery
from pydmc.core.mag import MagTools
from pydmc.core.struc import StrucTools
from pydmc.hpc.launch import LaunchTools
from pydmc.hpc.submit import SubmitTools
from pydmc.hpc.analyze import AnalyzeBatch

"""
This demo is really just to make sure that everything works using default arguments ...
We'll calculate a couple NiO structures to see

"""
"""
Overall note:
    - some of the configurations I'm choosing are purely to illustrate how to change things
        - not necessarily the best choices for a given calculation

"""

############### GLOBAL VARIABLES ################
"""
We'll define a few global variables here ####
 - note: generally we don't want to do this, 
    - but it's convenient for a few things (
    - e.g., specifying where to save data generated by many functions)
"""

# where is this file
SCRIPTS_DIR = os.getcwd()

# where are my calculations going to live
CALCS_DIR = SCRIPTS_DIR.replace('scripts', 'calcs')

# where is my data going to live
DATA_DIR = SCRIPTS_DIR.replace('scripts', 'data')

for d in [CALCS_DIR, DATA_DIR]:
    if not os.path.exists(d):
        os.makedirs(d)

# if you need data from MP as a starting point (often the case), you need your API key
API_KEY = 'N3KdATtMmcsUL94g'

# lets put a tag on all the files we save
FILE_TAG = 'defaults'

############### QUERYING MP ################

"""
One of the main things you must provide for this workflow is a starting crystal structure
    - oftentimes, we use Materials Project to grab initial structures
    - we then either calculate them directly using different approaches
    - or we manipulate them to make new structures
    
If we want MP data, we use pydmc.core.query.MPQuery
""" 

"""
Using default options for NiO

This function will return something that looks like this:
    {mpid (str, mp-****) : 
        {'cmpd' : chemical formula (str),
        'structure : Structure.as_dict(),
        'E_mp' : energy per atom from MP,
        'Ef_mp' : formation energy per atom from MP,
        etc}
"""
def get_query(comp='NiO',
              savename='query_%s.json' % FILE_TAG,
              remake=False):
    
    fjson = os.path.join(DATA_DIR, savename)
    if os.path.exists(fjson) and not remake:
       return read_json(fjson)
    
    mpq = MPQuery(api_key=API_KEY)
    
    data = mpq.get_data_for_comp(comp=comp,
                                 include_structure=True)
    
    write_json(data, fjson) 
    return read_json(fjson)   

"""
It's generally a good idea to write a quick "check" function to make sure each function does what you want
"""
def check_query(query):
    for mpid in query:
        print('\nmpid: %s' % mpid)
        print('\tcmpd: %s' % query[mpid]['cmpd'])
        print('\tstructure has %i sites' % len(StrucTools(query[mpid]['structure']).structure))
    
############### MAGMOMS ################
"""
Using defaults --> no AFMS --> no MAGMOMS
"""      
            
############### LAUNCHING ################
"""
We have our structures

Now, we can let pydmc set up all our calculations (once we specify some configurations)

In the first step, we'll figure out which "launch directories" we need to make and which VASP calculations will belong to each launch directory
    - in general, a VASP calculation is defined by a distinct:
        - POSCAR: the starting crystal structure
        - INCAR: the input parameters for VASP
        - KPOINTS: the k-point mesh
        - POTCAR: the pseudopotentials
    - oftentimes, we'll chain calculations together
        - for instance, we might run a geometry optimization ("relax") then a static calculation at that geometry ("static")
            - these "chains" will get "launched" sequentially
            - this will happen automatically so that the static starts only once the relax is finished
            - we will also pass information (input/output files) between these chained calculations that belong to a single launch directory
    - each launch directory will have one chain of calculations

This function will return something that looks like:
    {launch directory name (str, top_level/ID/standard/final_xc/mag/) 
        : [list of VASP calculations to run (e.g., gga-relax)]}
        
Note on launch_directory naming:
    - top_level = chemical formula or similar that describes a group of structures
    - ID = unique ID that describes the structure
    - standard = pydmc standards to generate input files
    - final_xc = the final exchange-correlation method we want to get energies for
    - mag = nm, fm, or afm_*
    
Note on user_configs:
    - we're using defaults here, so will leave empty
"""

def get_launch_dirs(query,
                    user_configs={},
                    magmoms=None,
                    make_launch_dirs=True,
                    savename='launch_dirs_%s.json' % FILE_TAG,
                    remake=False):
    
    fjson = os.path.join(DATA_DIR, savename)
    if os.path.exists(fjson) and not remake:
        return read_json(fjson)
    
    all_launch_dirs = {}
    for mpid in query:

        structure = query[mpid]['structure']
        top_level = query[mpid]['cmpd']
        ID = mpid
        
        launch = LaunchTools(calcs_dir=CALCS_DIR,
                             structure=structure,
                             top_level=top_level,
                             unique_ID=ID,
                             user_configs=user_configs,
                             magmoms=magmoms)

        launch_dirs = launch.launch_dirs_to_tags

        if make_launch_dirs:
            launch.create_launch_dirs_and_make_POSCARs

        all_launch_dirs = {**all_launch_dirs, **launch_dirs}

    write_json(all_launch_dirs, fjson) 
    return read_json(fjson)  

def check_launch_dirs(launch_dirs):
    print('\nanalyzing launch directories')
    for d in launch_dirs:
        print('\nlaunching from %s' % d)
        print('   these calcs: %s' % launch_dirs[d])

############### SUBMISSION ################
"""
Now that we have our launch directories,
    - we can prepare each VASP calculation 
    - write a submission script to launch each chain of calculations
    - submit the calculations to the queue

This function won't return anything, but it will submit jobs to the queue

Note on user_configs:
    - we're using defaults here, so will leave empty
"""

def submit_calcs(launch_dirs,
                 magmoms=None,
                 user_configs={},
                 ready_to_launch=True):
    
    for launch_dir in launch_dirs:

        # these are calcs that should be chained in this launch directory
        valid_calcs = launch_dirs[launch_dir]

        # these are some configurations we'll extract from the launch directory name
        top_level, ID, standard, final_xc, mag = launch_dir.split('/')[-5:]
        user_configs['standard'] = standard
        user_configs['xc'] = final_xc
        user_configs['mag'] = mag
        user_configs['job-name'] = '.'.join([top_level, ID, standard, final_xc, mag])

        # we need to pass the right MAGMOM if the calc is AFM
        if 'afm' in mag:
            idx = mag.split('_')[1]
            if ID in magmoms:
                magmom = magmoms[ID][idx]
            elif top_level in magmoms:
                magmom = magmoms[top_level][ID][idx]
        else:
            magmom = None

        user_configs['magmom'] = magmom
        
        # now we'll prep the VASP directories and write the submission script
        sub = SubmitTools(launch_dir=launch_dir,
                          valid_calcs=valid_calcs,
                          user_configs=user_configs)

        sub.write_sub
        
        # if we're "ready to launch", let's launch
        if ready_to_launch:
            sub.launch_sub    
            
def check_subs(launch_dirs,
                fsub='sub.sh'):
    print('\nanalyzing submission scripts')
    launch_dirs_to_check = list(launch_dirs.keys())
    if len(launch_dirs_to_check) > 6:
        launch_dirs_to_check = launch_dirs_to_check[:3] + launch_dirs_to_check[-3:]

    for d in launch_dirs_to_check:
        submission_file = os.path.join(d, fsub)
        with open(submission_file) as f:
            print('\nanalyzing %s' % submission_file)
            for line in f:
                if 'working' in line:
                    print(line)
                    
                    
############### ANALYSIS ################
    
"""
The last step is to analyze the calculations as they run

Depending on what you're goal is, there are a lot of additional analyses that might be useful.

The goal of VASPOutputs, AnalyzeVASP, and AnalyzeBatch are to save a lot of the HPC data into
    more tractable json's that can be moved locally for more analysis
    
This function will return a dictionary that looks like:
    {top_level.ID.standard.final_xc.mag.xc_calc: 
        {OUTPUT DATA} for calculations in launch_dir}
Note on user_configs:
    - we'll need to specify these if we want anything beside the default configurations
        - defaults for what to analyze can be found at pydmc/data/data/_batch_vasp_analysis_configs.yaml
    - do we want magnetization, DOS, etc.
    - do we only want static calculations, etc
    - set these in main()
"""

def analyze_calcs(launch_dirs,
                  user_configs,
                  savename='results_%s.json' % FILE_TAG,
                  remake=False):
    
    fjson = os.path.join(DATA_DIR, savename)
    if os.path.exists(fjson) and not remake:
        return read_json(fjson)
    
    analyzer = AnalyzeBatch(launch_dirs,
                            user_configs=user_configs)

    data = analyzer.results

    write_json(data, fjson)   
    return read_json(fjson)      

def check_results(results):

    keys_to_check = list(results.keys())

    converged = 0
    for key in keys_to_check:
        top_level, ID, standard, xc, mag, xc_calc = key.split('.')
        data = results[key]
        convergence = results[key]['results']['convergence']
        print('\n%s' % key)
        print('convergence = %s' % convergence)
        if convergence:
            converged += 1
            print('E (static) = %.2f' % data['results']['E_per_at'])

    
    print('\n\n %i/%i converged' % (converged, len(keys_to_check)))       
            
def main():
    """
    It's generally a good idea to set True/False statements at the top
        - this will allow you to quickly toggle whether or not to repeat certain steps
    """    
    remake_sub_launch = False
    
    remake_query = False
    print_query_check = True 
    
    remake_launch_dirs = False
    print_launch_dirs_check = True
    
    remake_subs = True
    ready_to_launch = True
    print_subs_check = True
    
    remake_results = True
    print_results_check = True
    
    """
    Sometimes we'll need to run our launch script on a compute node if generating magmoms or analyzing directories takes a while
        here, we'll create a file called sub_launch.sh
        you can then execute this .py file on a compute node with:
            $ sbatch sub_launchs.h
    """   
    if remake_sub_launch or not os.path.exists(os.path.join(os.getcwd(), 'sub_launch.sh')):
        make_sub_for_launcher()

    query = get_query(remake=remake_query)

    if print_query_check:
        check_query(query=query)
    
    """
    Here, we'll use defaults
    """
    launch_configs = {}
    
    launch_dirs = get_launch_dirs(query=query,
                                  user_configs=launch_configs,
                                  remake=remake_launch_dirs)
    if print_launch_dirs_check:
        check_launch_dirs(launch_dirs=launch_dirs)
        
    """
    Here, we'll use defaults
        
    """
    user_configs = {}
    
    if remake_subs:
        submit_calcs(launch_dirs=launch_dirs,
                     user_configs=user_configs,
                     ready_to_launch=ready_to_launch)
 
    if print_subs_check:
        check_subs(launch_dirs=launch_dirs)
    
    """
    Here, we'll use defaults
    
    """
    analysis_configs = {}
    results = analyze_calcs(launch_dirs=launch_dirs,
                            user_configs=analysis_configs,
                            remake=remake_results)
    if print_results_check:
        check_results(results)

if __name__ == '__main__':
    main()

