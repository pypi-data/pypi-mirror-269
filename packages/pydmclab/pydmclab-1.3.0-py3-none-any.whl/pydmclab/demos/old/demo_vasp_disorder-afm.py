import os
import numpy as np

from pydmc.utils.handy import read_json, write_json, make_sub_for_launcher
from pydmc.core.query import MPQuery
from pydmc.core.mag import MagTools
from pydmc.core.struc import StrucTools
from pydmc.hpc.launch import LaunchTools
from pydmc.hpc.submit import SubmitTools
from pydmc.hpc.analyze import AnalyzeBatch

"""
This demo:
    - Goal: compute the magnetic properties and energetics of RuO2-IrO2 alloys
    - Approach:
        1) Get the ground-state structure for RuO2 from MP
        2) Make it a supercell to allow for sampling configurations
        3) Enumerate some plausible Ru/Ir orderings at various x in Ru_{1-x}}Ir_{x}O2
        4) Enumerate some plauslabe AFM orderings for each Ru/Ir ordering for each x
        5) Calculate these with R2SCAN
        6) Collect the results        
        
Overall note:
    - some of the configurations I'm choosing are purely to illustrate how to change things
        - not necessarily the best choices for a given calculation
"""

############### GLOBAL VARIABLES ################
"""
We'll define a few global variables here ####
 - note: generally we don't want to do this, 
    - but it's convenient for a few things
    - e.g., specifying where to save data generated by many functions
"""

# where is this file
SCRIPTS_DIR = os.getcwd()

# where are my calculations going to live
CALCS_DIR = SCRIPTS_DIR.replace('scripts', 'calcs')

# where is my data going to live
DATA_DIR = SCRIPTS_DIR.replace('scripts', 'data')

for d in [CALCS_DIR, DATA_DIR]:
    if not os.path.exists(d):
        os.makedirs(d)

# if you need data from MP as a starting point (often the case), you need your API key
API_KEY = 'N3KdATtMmcsUL94g'

# lets put a tag on all the files we save
FILE_TAG = 'disorder-afm'

############### QUERYING MP ################

"""
One of the main things you must provide for this workflow is a starting crystal structure
    - oftentimes, we use Materials Project to grab initial structures
    - we then either calculate them directly using different approaches
    - or we manipulate them to make new structures
    
If we want MP data, we use pydmc.core.query.MPQuery
""" 

"""
This function will return something that looks like this:
    {mpid (str, mp-****) : 
        {'cmpd' : chemical formula (str),
        'structure : Structure.as_dict(),
        'E_mp' : energy per atom from MP,
        'Ef_mp' : formation energy per atom from MP,
        etc}
"""
def get_query(comp='RuO2',
              only_gs=True,
              include_structure=True,
              supercell_structure=[1,2,3],
              savename='query_%s.json' % FILE_TAG,
              remake=False):
    
    fjson = os.path.join(DATA_DIR, savename)
    if os.path.exists(fjson) and not remake:
       return read_json(fjson)
    
    mpq = MPQuery(api_key=API_KEY)
    
    data = mpq.get_data_for_comp(comp=comp,
                                 only_gs=only_gs,
                                 include_structure=include_structure,
                                 supercell_structure=supercell_structure)
    
    return write_json(data, fjson) 

############### STRUCTURE ENUMERATION ################

"""
It's generally a good idea to write a quick "check" function to make sure each function does what you want
"""
def check_query(query):
    for mpid in query:
        print('\nmpid: %s' % mpid)
        print('\tcmpd: %s' % query[mpid]['cmpd'])
        print('\tstructure has %i sites' % len(StrucTools(query[mpid]['structure']).structure))

"""
This first query just gave us our starting structure (x = 0 in Ru_{1-x}Ir_{x}O2)

Let's say we want to consider various Ir concentrations (x = 0, 1/4, 1/2, 3/4, 1)
    - this will lead to "disordered" structures --> those with partial occupancy of Ru and Ir on each cation site
    - we will use StrucTools to find low-electrostatic energy configurations at each x
    - then we'll calculate a small number of the low-energy ones
    
Note 1: having floats as keys is a little troublesome, so we'll convert them to ints
    x = 1/4 --> x = 1
    x = 2/4 --> x = 2 ...
Note 2: we'll then convert them to str because json doesn't like ints as keys

This function will return a dictionary of the form:
    {x (str, 1, 2, 3, ...) : 
        {index for structure (int) : structure.as_dict()}}
    - note: index = 0 will be lowest electrostatic energy structure
    - note: max_strucs_per_x = 2 means we'll only calculate the lowest two structures
    - note: x_values = [ list of x values to iterate over in Ru_{1-x}Ir_{x}O2 ]
    - note: ox_states don't need to be specified but they speed up / improve structure enumeration
"""

def get_strucs(query,
               x_values=[i/4 for i in range(5)],
               max_strucs_per_x=2,
               ox_states={'Ru' : 4, 'Ir' : 4, 'O' : -2},
               savename='initial_strucs_%s.json' % FILE_TAG,
               remake=False):

    fjson = os.path.join(DATA_DIR, savename)
    if os.path.exists(fjson) and not remake:
       return read_json(fjson)
   
    mpids_in_query = list(query.keys())
    if len(mpids_in_query) > 1:
        raise ValueError('should only have one key ...')
    mpid = mpids_in_query[0]
    initial_structure = query[mpid]['structure']
    data = {}
    for x in x_values:
        if x == 0:
            strucs = {0 : initial_structure}
        else:
            structools = StrucTools(structure=initial_structure,
                                    ox_states=ox_states)
            species_mapping = {'Ru' : {'Ru' : 1-x,
                                       'Ir' : x}}
            strucs = structools.replace_species(species_mapping=species_mapping,
                                                n_strucs=max_strucs_per_x)
        data[int((len(x_values)-1)*x)] = strucs
    write_json(data, fjson)
    return read_json(fjson)

def check_strucs(strucs):
    for x in strucs:
        print('\n')
        print('x = %.4f' % float(int(x)/4))
        print('%i strucs' % len(strucs[x]))
        for key in strucs[x]:
            print(StrucTools(strucs[x][key]).formula)
    
############### MAGMOMS ################
"""
Oftentimes we're interested in magnetic (spin-polarized) calculations

In the pydmc language, we have three types of initial magnetic configurations:
    - "nm" = nonmagnetic (not spin-polarized)
    - "fm" = ferromagnetic (all spins aligned in the positive direction)
    - "afm_*" = antiferromagnetic (half spins aligned in the positive direction, half in the negative direction)
        - you can imagine that there are many unique ways to make an afm configuration
        - the "*" is a placeholder for an integer (configuration 0, 1, 2, 3 = afm_0, afm_1, afm_2, afm_3)

If we're only interested in nm or fm calculations, you don't have to do anything as pydmc takes care of it all

If we want afm configurations, these are nontrivial to enumerate, so we want to enumerate them once and save to a file.

We'll generate a "magmoms" dictionary for some afm configurations for the compounds in query_example_1

Sometimes when we enumerate possible magnetic configurations, there are an absurdly large number
    - usually it's ok if we just sample some representative set of ~1-10 configurations
    
If we want to run VASP on N configurations, it's probably a good idea to set 
    - max_afm_combos at least 3*N in case some we generate are symmetrically equivalent
    
This function will return something that looks like:
    magmoms = {x (str, x/8 is fraction of Ir) : 
                {index of ordered structure (int) : 
                    {AFM configuration index (int) :
                        [magmoms (list of floats, len = # of sites)]}} }
                        
Note:
    - unlike in simple-afm, where our "unique_ID" was an MPID, and we had only one structure for each ID,
        - here we have x values which are like our "formula"s 
            - and each formul has the same kind of IDs (0, 1, 2, 3) correspondign to low energy ordered structures          

    
"""

def get_magmoms(strucs,
                max_afm_combos=15,
                savename='magmoms_%s.json' % FILE_TAG,
                remake=False):
                          
    fjson = os.path.join(DATA_DIR, savename)
    if not remake and os.path.exists(fjson):
        return read_json(fjson)
    
    magmoms = {}
    for x in strucs:
        magmoms[str(x)] = {}
        for i in strucs[x]:
            magmoms[x][i] = {}
            structure = strucs[x][i]
            magtools = MagTools(structure=structure,
                                max_afm_combos=max_afm_combos)
            curr_magmoms = magtools.get_afm_magmoms
            magmoms[x][i] = curr_magmoms

    write_json(magmoms, fjson)
    return read_json(fjson)

def check_magmoms(strucs,
                  magmoms):
    """
    simple function to inspect the magmoms that were generated
    """
    for x in strucs:
        for i in strucs[x]:
            cmpd = StrucTools(strucs[x][i]).formula
            curr_magmoms = magmoms[x][i]
            if curr_magmoms:
                print('%s looks good' % cmpd)
            if not curr_magmoms:
                print('!!! %s has no MAGMOM' % cmpd)          
            
############### LAUNCHING ################
"""
We have our structures and we have their MAGMOMs (because it's AFM)

Now, we can let pydmc set up all our calculations (once we specify some configurations)

In the first step, we'll figure out which "launch directories" we need to make and which VASP calculations will belong to each launch directory
    - in general, a VASP calculation is defined by a distinct:
        - POSCAR: the starting crystal structure
        - INCAR: the input parameters for VASP
        - KPOINTS: the k-point mesh
        - POTCAR: the pseudopotentials
    - oftentimes, we'll chain calculations together
        - for instance, we might run a geometry optimization ("relax") then a static calculation at that geometry ("static")
            - these "chains" will get "launched" sequentially
            - this will happen automatically so that the static starts only once the relax is finished
            - we will also pass information (input/output files) between these chained calculations that belong to a single launch directory
    - each launch directory will have one chain of calculations

This function will return something that looks like:
    {launch directory name (str, top_level/ID/standard/final_xc/mag/) 
        : [list of VASP calculations to run (e.g., gga-relax)]}
        
Note on launch_directory naming:
    - top_level = chemical formula or similar that describes a group of structures (here it will be "x")
    - ID = unique ID that describes the structure (here it will be the index from ordering structures at each x)
    - standard = pydmc standards to generate input files
    - final_xc = the final exchange-correlation method we want to get energies for
    - mag = nm, fm, or afm_*
    
Note on user_configs:
    - we'll need to specify these if we want anything beside the default configuration
        - defaults found at pydmc/data/data/_launch_configs.yaml
    - set these in main()
"""

def get_launch_dirs(strucs,
                    user_configs,
                    magmoms,
                    make_launch_dirs=True,
                    savename='launch_dirs_%s.json' % FILE_TAG,
                    remake=False):
    
    fjson = os.path.join(DATA_DIR, savename)
    if os.path.exists(fjson) and not remake:
        return read_json(fjson)
    
    all_launch_dirs = {}
    for x in strucs:
        for i in strucs[x]:
            structure = strucs[x][i]
            curr_magmoms = magmoms[x][i]
            top_level = str(x)
            ID = str(i)
        
            launch = LaunchTools(calcs_dir=CALCS_DIR,
                                structure=structure,
                                magmoms=curr_magmoms,
                                top_level=top_level,
                                unique_ID=ID,
                                user_configs=user_configs)
            
            launch_dirs = launch.launch_dirs_to_tags

            if make_launch_dirs:
                launch.create_launch_dirs_and_make_POSCARs

            all_launch_dirs = {**all_launch_dirs, **launch_dirs}

    write_json(all_launch_dirs, fjson)   
    return read_json(fjson)

def check_launch_dirs(launch_dirs):
    print('\nanalyzing launch directories')
    for d in launch_dirs:
        print('\nlaunching from %s' % d)
        print('   these calcs: %s' % launch_dirs[d])

############### SUBMISSION ################
"""
Now that we have our launch directories,
    - we can prepare each VASP calculation 
    - write a submission script to launch each chain of calculations
    - submit the calculations to the queue

This function won't return anything, but it will submit jobs to the queue

Note on user_configs:
    - we'll need to specify these if we want anything beside the default configurations
        - defaults for submission parameters found at pydmc/data/data/_sub_configs.yaml
        - defaults for slurm parameters found at pydmc/data/data/_slurm_configs.yaml
        - defaults for VASP parameters found at pydmc/data/data/_vasp_configs.yaml
    - set these in main()
"""

def submit_calcs(launch_dirs,
                 user_configs,
                 magmoms,
                 ready_to_launch=True):
    for launch_dir in launch_dirs:

        # these are calcs that should be chained in this launch directory
        valid_calcs = launch_dirs[launch_dir]

        # these are some configurations we'll extract from the launch directory name
        top_level, ID, standard, final_xc, mag = launch_dir.split('/')[-5:]
        user_configs['standard'] = standard
        user_configs['xc'] = final_xc
        user_configs['mag'] = mag
        user_configs['job-name'] = '.'.join([top_level, ID, standard, final_xc, mag])

        # we need to pass the right MAGMOM if the calc is AFM
        if 'afm' in mag:
            idx = mag.split('_')[1]
            magmom = magmoms[top_level][ID][idx]
        else:
            magmom = None

        user_configs['magmom'] = magmom
        
        # now we'll prep the VASP directories and write the submission script
        sub = SubmitTools(launch_dir=launch_dir,
                          valid_calcs=valid_calcs,
                          user_configs=user_configs)

        sub.write_sub
        
        # if we're "ready to launch", let's launch
        if ready_to_launch:
            sub.launch_sub    
            
def check_subs(launch_dirs,
                fsub='sub.sh'):
    print('\nanalyzing submission scripts')
    launch_dirs_to_check = list(launch_dirs.keys())
    if len(launch_dirs_to_check) > 6:
        launch_dirs_to_check = launch_dirs_to_check[:3] + launch_dirs_to_check[-3:]

    for d in launch_dirs_to_check:
        submission_file = os.path.join(d, fsub)
        with open(submission_file) as f:
            print('\nanalyzing %s' % submission_file)
            for line in f:
                if 'working' in line:
                    print(line)
                    
                    
############### ANALYSIS ################
    
"""
The last step is to analyze the calculations as they run

Depending on what you're goal is, there are a lot of additional analyses that might be useful.

The goal of VASPOutputs, AnalyzeVASP, and AnalyzeBatch are to save a lot of the HPC data into
    more tractable json's that can be moved locally for more analysis
    
This function will return a dictionary that looks like:
    {top_level.ID.standard.final_xc.mag.xc_calc: 
        {OUTPUT DATA} for calculations in launch_dir}

Note on user_configs:
    - we'll need to specify these if we want anything beside the default configurations
        - defaults for what to analyze can be found at pydmc/data/data/_batch_vasp_analysis_configs.yaml
    - do we want magnetization, DOS, etc.
    - do we only want static calculations, etc
    - set these in main()
"""

def analyze_calcs(launch_dirs,
                  user_configs,
                  savename='results_%s.json' % FILE_TAG,
                  remake=False):
    
    fjson = os.path.join(DATA_DIR, savename)
    if os.path.exists(fjson) and not remake:
        return read_json(fjson)
    
    analyzer = AnalyzeBatch(launch_dirs,
                            user_configs=user_configs)

    data = analyzer.results

    write_json(data, fjson) 
    return read_json(fjson)  

def check_results(results):

    keys_to_check = list(results.keys())

    converged = 0
    for key in keys_to_check:
        top_level, ID, standard, xc, mag, xc_calc = key.split('.')
        data = results[key]
        convergence = results[key]['results']['convergence']
        print('\n%s' % key)
        print('convergence = %s' % convergence)
        if convergence:
            converged += 1
            print('E (static) = %.2f' % data['results']['E_per_at'])
            print('E (relax) = %.2f' % data['meta']['E_relax'])
            print('EDIFFG = %i' % data['meta']['incar']['EDIFFG'])
            print('1st POTCAR = %s' % data['meta']['potcar'][0])
            if mag != 'nm':
                magnetization = data['magnetization']
                an_el = list(magnetization.keys())[0]
                an_idx = list(magnetization[an_el].keys())[0]
                that_mag = magnetization[an_el][an_idx]['mag']
                print('mag on %s (%s) = %.2f' % (an_el, str(an_idx), that_mag))
            print(data['structure'])
    
    print('\n\n %i/%i converged' % (converged, len(keys_to_check)))       
            
def main():
    """
    It's generally a good idea to set True/False statements at the top
        - this will allow you to quickly toggle whether or not to repeat certain steps
    """    
    remake_sub_launch = False
    
    remake_query = False
    print_query_check = True 
    
    remake_strucs = False
    print_strucs_check = True

    remake_magmoms = False
    print_magmoms_check = True
    
    remake_launch_dirs = False
    print_launch_dirs_check = True
    
    remake_subs = True
    ready_to_launch = True
    print_subs_check = True
    
    remake_results = True
    print_results_check = True
    
    """
    Sometimes we'll need to run our launch script on a compute node if generating magmoms or analyzing directories takes a while
        here, we'll create a file called sub_launch.sh
        you can then execute this .py file on a compute node with:
            $ sbatch sub_launchs.h
    """   
    if remake_sub_launch or not os.path.exists(os.path.join(os.getcwd(), 'sub_launch.sh')):
        make_sub_for_launcher()

    query = get_query(remake=remake_query)

    if print_query_check:
        check_query(query=query)
        
    strucs = get_strucs(query=query,
                        remake=remake_strucs)

    if print_strucs_check:
        check_strucs(strucs=strucs)

    magmoms = get_magmoms(strucs=strucs,
                          remake=remake_magmoms)

    if print_magmoms_check:
        check_magmoms(strucs=strucs,
                      magmoms=magmoms)
    
    """
    Here, I'll specify the user_configs pertaining to setting up the launch directories
        - let's consider 4 AFM configurations
        - let's use DMC standards
        - and let's use GGA
    """
    launch_configs = {'n_afm_configs' : 2,
                      'standards' : ['dmc'],
                      'final_xcs' : ['gga']}
    
    launch_dirs = get_launch_dirs(strucs=strucs,
                                  user_configs=launch_configs,
                                  magmoms=magmoms,                                  
                                  remake=remake_launch_dirs)
    if print_launch_dirs_check:
        check_launch_dirs(launch_dirs=launch_dirs)
        
    """
    Now, we need to specify any configurations relevant to VASP set up or our submission scripts
    For this example, we'll do the following (on top of the defaults):
        - run on only 8 cores
        - run with a walltime of 23 hours
        - make sure we don't run LOBSTER
        - turn off symmetry in our loose, relax, and static calculations
        
    """
    user_configs = {'ntasks' : 8,
                    'time' : int(23*60),
                    'lobster_static' : False}
    
    if remake_subs:
        submit_calcs(launch_dirs=launch_dirs,
                     user_configs=user_configs,
                     magmoms=magmoms,                    
                     ready_to_launch=ready_to_launch)
 
    if print_subs_check:
        check_subs(launch_dirs=launch_dirs)
        
    """
    Now, we can specify what we want to collect from our calculations
        - let's run in parallel w/ 4 processors
        - include metadata
        - include magnetization results
        
    """
    
    analysis_configs = {'n_procs' : 4,
                        'include_meta' : True,
                        'include_mag' : True}
    results = analyze_calcs(launch_dirs=launch_dirs,
                            user_configs=analysis_configs,
                            remake=remake_results)
    if print_results_check:
        check_results(results)

if __name__ == '__main__':
    main()

